"""CLI ì¸í„°í˜ì´ìŠ¤."""

import argparse
import json
import logging
import os
import shutil
import sys
import tempfile
from dataclasses import dataclass
from pathlib import Path

from tubearchive.core.detector import detect_metadata
from tubearchive.core.merger import Merger
from tubearchive.core.scanner import scan_videos
from tubearchive.core.transcoder import Transcoder
from tubearchive.database.repository import MergeJobRepository
from tubearchive.database.schema import init_database
from tubearchive.utils.progress import MultiProgressBar

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
ENV_OUTPUT_DIR = "TUBEARCHIVE_OUTPUT_DIR"


def get_default_output_dir() -> Path | None:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°."""
    env_dir = os.environ.get(ENV_OUTPUT_DIR)
    if env_dir:
        path = Path(env_dir)
        if path.is_dir():
            return path
        logger.warning(f"{ENV_OUTPUT_DIR}={env_dir} is not a valid directory")
    return None


def get_temp_dir() -> Path:
    """ì‹œìŠ¤í…œ ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ tubearchive í´ë” ë°˜í™˜."""
    temp_base = Path(tempfile.gettempdir()) / "tubearchive"
    temp_base.mkdir(exist_ok=True)
    return temp_base


def check_output_disk_space(output_dir: Path, required_bytes: int) -> bool:
    """
    ì¶œë ¥ ë””ë ‰í† ë¦¬ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸.

    Args:
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        required_bytes: í•„ìš”í•œ ë°”ì´íŠ¸ ìˆ˜

    Returns:
        ê³µê°„ì´ ì¶©ë¶„í•˜ë©´ True
    """
    usage = shutil.disk_usage(output_dir)
    if usage.free < required_bytes:
        logger.warning(
            f"Insufficient disk space: {usage.free / (1024**3):.1f}GB available, "
            f"{required_bytes / (1024**3):.1f}GB required"
        )
        return False
    return True


@dataclass
class ValidatedArgs:
    """ê²€ì¦ëœ CLI ì¸ì."""

    targets: list[Path]
    output: Path | None
    output_dir: Path | None
    no_resume: bool
    keep_temp: bool
    dry_run: bool


def create_parser() -> argparse.ArgumentParser:
    """
    CLI íŒŒì„œ ìƒì„±.

    Returns:
        argparse.ArgumentParser ì¸ìŠ¤í„´ìŠ¤
    """
    parser = argparse.ArgumentParser(
        prog="tubearchive",
        description="ë‹¤ì–‘í•œ ê¸°ê¸°ì˜ 4K ì˜ìƒì„ í‘œì¤€í™”í•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤.",
        epilog="ì˜ˆì‹œ: tubearchive video1.mp4 video2.mov -o merged.mp4",
    )

    parser.add_argument(
        "targets",
        nargs="*",
        default=[],
        help="ì˜ìƒ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ (ê¸°ë³¸: í˜„ì¬ ë””ë ‰í† ë¦¬)",
    )

    parser.add_argument(
        "-o", "--output",
        type=str,
        default=None,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: merged_output.mp4)",
    )

    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="Resume ê¸°ëŠ¥ ë¹„í™œì„±í™”",
    )

    parser.add_argument(
        "--keep-temp",
        action="store_true",
        help="ì„ì‹œ íŒŒì¼ ë³´ì¡´ (ë””ë²„ê¹…ìš©)",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥ (ì‹¤ì œ ì‹¤í–‰ ì•ˆ í•¨)",
    )

    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help=f"ì¶œë ¥ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ (í™˜ê²½ë³€ìˆ˜: {ENV_OUTPUT_DIR})",
    )

    return parser


def validate_args(args: argparse.Namespace) -> ValidatedArgs:
    """
    CLI ì¸ì ê²€ì¦.

    Args:
        args: íŒŒì‹±ëœ ì¸ì

    Returns:
        ê²€ì¦ëœ ì¸ì

    Raises:
        FileNotFoundError: íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    """
    # targets ê²€ì¦
    targets: list[Path] = []
    if not args.targets:
        targets = [Path.cwd()]
    else:
        for target in args.targets:
            path = Path(target)
            if not path.exists():
                raise FileNotFoundError(f"Target not found: {target}")
            targets.append(path)

    # output ê²€ì¦
    output: Path | None = None
    if args.output:
        output = Path(args.output)
        if not output.parent.exists():
            raise FileNotFoundError(f"Output directory not found: {output.parent}")

    # output_dir ê²€ì¦ (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > None)
    output_dir: Path | None = None
    if args.output_dir:
        output_dir = Path(args.output_dir)
        if not output_dir.is_dir():
            raise FileNotFoundError(f"Output directory not found: {args.output_dir}")
    else:
        output_dir = get_default_output_dir()

    return ValidatedArgs(
        targets=targets,
        output=output,
        output_dir=output_dir,
        no_resume=args.no_resume,
        keep_temp=args.keep_temp,
        dry_run=args.dry_run,
    )


def setup_logging(verbose: bool = False) -> None:
    """
    ë¡œê¹… ì„¤ì •.

    Args:
        verbose: ìƒì„¸ ë¡œê·¸ ì—¬ë¶€
    """
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def get_output_filename(targets: list[Path]) -> str:
    """
    ì…ë ¥ íƒ€ê²Ÿì—ì„œ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±.

    ë””ë ‰í† ë¦¬ëª… ë˜ëŠ” ì²« ë²ˆì§¸ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ëª…ì„ ì‚¬ìš©.

    Args:
        targets: ì…ë ¥ íƒ€ê²Ÿ ëª©ë¡

    Returns:
        ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
    """
    if not targets:
        return "output.mp4"

    first_target = targets[0]
    if first_target.is_dir():
        # ë””ë ‰í† ë¦¬ë©´ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        name = first_target.name
    else:
        # íŒŒì¼ì´ë©´ ë¶€ëª¨ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        name = first_target.parent.name

    # ë¹ˆ ì´ë¦„ì´ê±°ë‚˜ í˜„ì¬ ë””ë ‰í† ë¦¬ë©´ ê¸°ë³¸ê°’
    if not name or name == ".":
        name = "output"

    return f"{name}.mp4"


def run_pipeline(validated_args: ValidatedArgs) -> Path:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.

    Args:
        validated_args: ê²€ì¦ëœ ì¸ì

    Returns:
        ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # 1. íŒŒì¼ ìŠ¤ìº”
    logger.info("Scanning video files...")
    video_files = scan_videos(validated_args.targets)

    if not video_files:
        logger.error("No video files found")
        raise ValueError("No video files found")

    logger.info(f"Found {len(video_files)} video files")
    for vf in video_files:
        logger.info(f"  - {vf.path.name}")

    # 2. íŠ¸ëœìŠ¤ì½”ë”© (ì„ì‹œ íŒŒì¼ì€ /tmpì— ì €ì¥)
    temp_dir = get_temp_dir()
    logger.info(f"Using temp directory: {temp_dir}")
    logger.info("Starting transcoding...")
    transcoded_paths: list[Path] = []
    # (íŒŒì¼ëª…, duration, device_model, creation_time_str)
    video_clips: list[tuple[str, float, str | None, str | None]] = []
    progress = MultiProgressBar(total_files=len(video_files))

    with Transcoder(temp_dir=temp_dir) as transcoder:
        for vf in video_files:
            progress.start_file(vf.path.name)

            def on_progress(percent: int) -> None:
                progress.update_file_progress(percent)

            output_path = transcoder.transcode_video(vf)
            transcoded_paths.append(output_path)

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (Summaryìš©)
            try:
                metadata = detect_metadata(vf.path)
                creation_time_str = vf.creation_time.strftime("%H:%M:%S")
                video_clips.append((
                    vf.path.name,
                    metadata.duration_seconds,
                    metadata.device_model,
                    creation_time_str,
                ))
            except Exception as e:
                logger.warning(f"Failed to get metadata for {vf.path}: {e}")
                video_clips.append((vf.path.name, 0.0, None, None))

            progress.finish_file()

    # 3. ë³‘í•©
    logger.info("Merging videos...")

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì •
    if validated_args.output:
        output_path = validated_args.output
    else:
        output_filename = get_output_filename(validated_args.targets)
        output_dir = validated_args.output_dir or Path.cwd()
        output_path = output_dir / output_filename

    merger = Merger(temp_dir=temp_dir)
    final_path = merger.merge(transcoded_paths, output_path)

    logger.info(f"Final output: {final_path}")

    # 4. DBì— íƒ€ì„ë¼ì¸ ì •ë³´ ì €ì¥ ë° Summary ìƒì„±
    summary_markdown = save_merge_job_to_db(
        final_path, video_clips, validated_args.targets
    )

    # 5. ì„ì‹œ íŒŒì¼ ë° í´ë” ì •ë¦¬
    if not validated_args.keep_temp:
        logger.info("Cleaning up temporary files...")
        for temp_path in transcoded_paths:
            if temp_path.exists() and temp_path != final_path:
                temp_path.unlink()
                logger.debug(f"  Removed: {temp_path}")

        # ì„ì‹œ í´ë” ì‚­ì œ (ë¹„ì–´ìˆê±°ë‚˜ concat íŒŒì¼ë§Œ ë‚¨ì€ ê²½ìš°)
        if temp_dir.exists():
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"Removed temp directory: {temp_dir}")
            except OSError as e:
                logger.warning(f"Failed to remove temp directory: {e}")

    # 6. Summary ì¶œë ¥ (ë³µì‚¬í•´ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    if summary_markdown:
        print("\n" + "=" * 60)
        print("ğŸ“‹ SUMMARY (Copy & Paste)")
        print("=" * 60)
        print(summary_markdown)
        print("=" * 60 + "\n")

    return final_path


def save_merge_job_to_db(
    output_path: Path,
    video_clips: list[tuple[str, float, str | None, str | None]],
    targets: list[Path],
) -> str | None:
    """
    ë³‘í•© ì‘ì—… ì •ë³´ë¥¼ DBì— ì €ì¥ (íƒ€ì„ë¼ì¸ ë° Summary í¬í•¨).

    Args:
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        video_clips: (íŒŒì¼ëª…, ì¬ìƒì‹œê°„, ê¸°ì¢…, ì´¬ì˜ì‹œê°„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        targets: ì…ë ¥ íƒ€ê²Ÿ ëª©ë¡ (ì œëª© ì¶”ì¶œìš©)

    Returns:
        ìƒì„±ëœ Summary ë§ˆí¬ë‹¤ìš´ (ì‹¤íŒ¨ ì‹œ None)
    """
    from tubearchive.utils.summary_generator import generate_clip_summary

    try:
        conn = init_database()
        repo = MergeJobRepository(conn)

        # íƒ€ì„ë¼ì¸ ì •ë³´ ìƒì„± (ê° í´ë¦½ì˜ ë©”íƒ€ë°ì´í„° í¬í•¨)
        timeline: list[dict[str, str | float | None]] = []
        current_time = 0.0
        for name, duration, device, shot_time in video_clips:
            timeline.append({
                "name": name,
                "duration": duration,
                "start": current_time,
                "end": current_time + duration,
                "device": device,
                "shot_time": shot_time,
            })
            current_time += duration

        clips_json = json.dumps(timeline, ensure_ascii=False)

        # ì œëª©: ë””ë ‰í† ë¦¬ëª…
        title = None
        if targets:
            first_target = targets[0]
            if first_target.is_dir():
                title = first_target.name
            else:
                title = first_target.parent.name
            if not title or title == ".":
                title = output_path.stem

        # ë‚ ì§œ: ì˜¤ëŠ˜
        from datetime import date
        today = date.today().isoformat()

        # ì´ ì¬ìƒì‹œê°„ ë° íŒŒì¼ í¬ê¸°
        total_duration = sum(d for _, d, _, _ in video_clips)
        total_size = output_path.stat().st_size if output_path.exists() else 0

        # Summary ë§ˆí¬ë‹¤ìš´ ìƒì„± (ê¸°ì¢…, ì´¬ì˜ì‹œê°„, íƒ€ì„ìŠ¤íƒ¬í”„)
        summary_markdown = generate_clip_summary(video_clips)

        repo.create(
            output_path=output_path,
            video_ids=[],
            title=title,
            date=today,
            total_duration_seconds=total_duration,
            total_size_bytes=total_size,
            clips_info_json=clips_json,
            summary_markdown=summary_markdown,
        )
        conn.close()
        logger.debug("Merge job saved to database with summary")
        return summary_markdown

    except Exception as e:
        logger.warning(f"Failed to save merge job to DB: {e}")
        return None


def main() -> None:
    """CLI ì§„ì…ì ."""
    parser = create_parser()
    args = parser.parse_args()

    setup_logging(args.verbose)

    try:
        validated_args = validate_args(args)

        if validated_args.dry_run:
            # Dry run: ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥
            logger.info("Dry run mode - showing execution plan only")

            video_files = scan_videos(validated_args.targets)
            temp_dir = get_temp_dir()

            # ì¶œë ¥ ê²½ë¡œ ê³„ì‚°
            if validated_args.output:
                output_str = str(validated_args.output)
            else:
                output_filename = get_output_filename(validated_args.targets)
                output_dir = validated_args.output_dir or Path.cwd()
                output_str = str(output_dir / output_filename)

            print("\n=== Dry Run Execution Plan ===")
            print(f"Input targets: {[str(t) for t in validated_args.targets]}")
            print(f"Video files found: {len(video_files)}")
            for vf in video_files:
                print(f"  - {vf.path}")
            print(f"Output: {output_str}")
            print(f"Temp dir: {temp_dir}")
            print(f"Resume enabled: {not validated_args.no_resume}")
            print(f"Keep temp files: {validated_args.keep_temp}")
            print("=" * 30)
            return

        output_path = run_pipeline(validated_args)
        print("\nâœ… ì™„ë£Œ!")
        print(f"ğŸ“¹ ì¶œë ¥ íŒŒì¼: {output_path}")

    except FileNotFoundError as e:
        logger.error(str(e))
        sys.exit(1)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)
    except KeyboardInterrupt:
        logger.info("\nInterrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
