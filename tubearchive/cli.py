"""TubeArchive CLI ì§„ì…ì .

ë‹¤ì–‘í•œ ê¸°ê¸°(Nikon, GoPro, DJI, iPhone)ì˜ 4K ì˜ìƒì„ HEVC 10-bitë¡œ
í‘œì¤€í™”Â·ë³‘í•©í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•œë‹¤.

íŒŒì´í”„ë¼ì¸ íë¦„::

    scan_videos â†’ Transcoder.transcode_video â†’ Merger.merge
    â†’ save_merge_job_to_db â†’ [í”„ë¡œì íŠ¸ ì—°ê²°] â†’ [upload_to_youtube]

ì£¼ìš” ì„œë¸Œì»¤ë§¨ë“œ:
    - ê¸°ë³¸(ì¸ì ì—†ìŒ): ì˜ìƒ ìŠ¤ìº” â†’ íŠ¸ëœìŠ¤ì½”ë”© â†’ ë³‘í•©
    - ``--project NAME``: ë³‘í•© ê²°ê³¼ë¥¼ í”„ë¡œì íŠ¸ì— ì—°ê²° (ìë™ ìƒì„±)
    - ``--project-list`` / ``--project-detail ID``: í”„ë¡œì íŠ¸ ê´€ë¦¬
    - ``--upload`` / ``--upload-only``: YouTube ì—…ë¡œë“œ
    - ``--status`` / ``--catalog``: ì‘ì—… í˜„í™©Â·ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    - ``--setup-youtube`` / ``--youtube-auth``: ì¸ì¦ ê´€ë¦¬
"""

import argparse
import json
import logging
import os
import re
import shutil
import sqlite3
import subprocess
import sys
import tempfile
from collections.abc import Generator, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import date, datetime
from pathlib import Path
from threading import Lock
from typing import TYPE_CHECKING, NamedTuple

from tubearchive import __version__

if TYPE_CHECKING:
    from tubearchive.notification.notifier import Notifier
from tubearchive.commands.catalog import (
    CATALOG_STATUS_SENTINEL,
    STATUS_ICONS,
    cmd_catalog,
    format_duration,
    normalize_status_filter,
)
from tubearchive.config import (
    ENV_FADE_DURATION,
    ENV_GROUP_SEQUENCES,
    ENV_OUTPUT_DIR,
    ENV_PARALLEL,
    ENV_YOUTUBE_PLAYLIST,
    get_default_auto_lut,
    get_default_bgm_loop,
    get_default_bgm_path,
    get_default_bgm_volume,
    get_default_denoise,
    get_default_denoise_level,
    get_default_fade_duration,
    get_default_group_sequences,
    get_default_normalize_audio,
    get_default_notify,
    get_default_output_dir,
    get_default_parallel,
    get_default_stabilize,
    get_default_stabilize_crop,
    get_default_stabilize_strength,
)
from tubearchive.core.detector import detect_metadata
from tubearchive.core.grouper import (
    FileSequenceGroup,
    compute_fade_map,
    group_sequences,
    reorder_with_groups,
)
from tubearchive.core.merger import Merger
from tubearchive.core.ordering import (
    SortKey,
    filter_videos,
    interactive_reorder,
    print_video_list,
    sort_videos,
)
from tubearchive.core.scanner import scan_videos
from tubearchive.core.splitter import probe_duration
from tubearchive.core.transcoder import Transcoder
from tubearchive.database.repository import (
    MergeJobRepository,
    SplitJobRepository,
    TranscodingJobRepository,
    VideoRepository,
)
from tubearchive.database.schema import init_database
from tubearchive.ffmpeg.effects import LUT_SUPPORTED_EXTENSIONS, SilenceSegment
from tubearchive.models.video import FadeConfig, VideoFile
from tubearchive.utils import truncate_path
from tubearchive.utils.progress import MultiProgressBar, ProgressInfo, format_size
from tubearchive.utils.summary_generator import generate_single_file_description

logger = logging.getLogger(__name__)

SUPPORTED_THUMBNAIL_EXTENSIONS = frozenset({".jpg", ".jpeg", ".png"})


# NOTE: STATUS_ICONS, CATALOG_STATUS_SENTINEL, format_duration, normalize_status_filter ë“±
#       ì¹´íƒˆë¡œê·¸/ìƒíƒœ ê´€ë ¨ ìƒìˆ˜ì™€ ìœ í‹¸ë¦¬í‹°ëŠ” tubearchive.commands.catalogì—ì„œ importí•©ë‹ˆë‹¤.


def safe_input(prompt: str) -> str:
    """
    í„°ë¯¸ë„ì—ì„œ ì•ˆì „í•˜ê²Œ ì…ë ¥ ë°›ê¸°.

    tmux ë“± í™˜ê²½ì—ì„œë„ ë™ì‘í•˜ë„ë¡ bash read ì‚¬ìš©.

    Args:
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸

    Returns:
        ì‚¬ìš©ì ì…ë ¥ (strip ì ìš©)
    """
    sys.stdout.write(prompt)
    sys.stdout.flush()

    try:
        # bash read ì‚¬ìš© (í„°ë¯¸ë„ ì„¤ì •ì— ëœ ë¯¼ê°)
        result = subprocess.run(
            ["bash", "-c", "read -r line </dev/tty && printf '%s' \"$line\""],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except (OSError, subprocess.SubprocessError):
        pass

    # fallback: ê¸°ë³¸ input
    try:
        return input().strip()
    except (EOFError, KeyboardInterrupt):
        return ""


# NOTE: í™˜ê²½ë³€ìˆ˜ ìƒìˆ˜(ENV_*)ì™€ ê¸°ë³¸ê°’ í—¬í¼(get_default_*)ëŠ”
#       tubearchive.config ëª¨ë“ˆì—ì„œ importí•©ë‹ˆë‹¤.


@contextmanager
def database_session() -> Generator[sqlite3.Connection]:
    """DB ì—°ê²°ì„ ìë™ìœ¼ë¡œ ë‹«ì•„ì£¼ëŠ” context manager.

    ``init_database()`` ë¡œ ì—°ê²°ì„ ì—´ê³ , ë¸”ë¡ì´ ëë‚˜ë©´ (ì˜ˆì™¸ ë°œìƒ í¬í•¨)
    ìë™ìœ¼ë¡œ ``conn.close()`` ë¥¼ í˜¸ì¶œí•œë‹¤.

    Yields:
        sqlite3.Connection: ì´ˆê¸°í™”ëœ DB ì—°ê²°
    """
    conn = init_database()
    try:
        yield conn
    finally:
        conn.close()


class ClipInfo(NamedTuple):
    """ì˜ìƒ í´ë¦½ ë©”íƒ€ë°ì´í„° (SummaryÂ·íƒ€ì„ë¼ì¸ìš©).

    ``_collect_clip_info`` ì˜ ë°˜í™˜ê°’ìœ¼ë¡œ, ê¸°ì¡´ ``tuple[str, float, str|None, str|None]``
    ì„ ëŒ€ì²´í•˜ì—¬ í•„ë“œ ì˜ë¯¸ë¥¼ ëª…í™•íˆ í•œë‹¤. NamedTupleì´ë¯€ë¡œ ê¸°ì¡´ tuple ì–¸íŒ¨í‚¹ê³¼
    ì—­í˜¸í™˜ëœë‹¤.

    Attributes:
        name: íŒŒì¼ëª… (ì˜ˆ: ``GH010042.MP4``)
        duration: ì¬ìƒì‹œê°„ (ì´ˆ)
        device: ì´¬ì˜ ê¸°ê¸°ëª… (ì˜ˆ: ``Nikon Z6III``, ``GoPro HERO12``)
        shot_time: ì´¬ì˜ ì‹œê° ë¬¸ìì—´ (``HH:MM:SS``, Noneì´ë©´ ì•Œ ìˆ˜ ì—†ìŒ)
    """

    name: str
    duration: float
    device: str | None
    shot_time: str | None


# YYYYMMDD íŒ¨í„´ (íŒŒì¼ëª… ì‹œì‘ ë¶€ë¶„)
DATE_PATTERN = re.compile(r"^(\d{4})(\d{2})(\d{2})\s*(.*)$")


def format_youtube_title(title: str) -> str:
    """
    YouTube ì œëª© í¬ë§·íŒ….

    YYYYMMDD í˜•ì‹ì˜ ë‚ ì§œë¥¼ 'YYYYë…„ Mì›” Dì¼'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: '20240115 ë„ì¿„ ì—¬í–‰' â†’ '2024ë…„ 1ì›” 15ì¼ ë„ì¿„ ì—¬í–‰'

    Args:
        title: ì›ë³¸ ì œëª©

    Returns:
        í¬ë§·íŒ…ëœ ì œëª©
    """
    match = DATE_PATTERN.match(title)
    if match:
        year, month, day, rest = match.groups()
        # ì•ì˜ 0 ì œê±° (01 â†’ 1)
        month_int = int(month)
        day_int = int(day)
        formatted = f"{year}ë…„ {month_int}ì›” {day_int}ì¼"
        if rest:
            formatted += f" {rest}"
        return formatted
    return title


def get_temp_dir() -> Path:
    """ì‹œìŠ¤í…œ ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ tubearchive í´ë” ë°˜í™˜."""
    temp_base = Path(tempfile.gettempdir()) / "tubearchive"
    temp_base.mkdir(exist_ok=True)
    return temp_base


def check_output_disk_space(output_dir: Path, required_bytes: int) -> bool:
    """
    ì¶œë ¥ ë””ë ‰í† ë¦¬ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸.

    Args:
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        required_bytes: í•„ìš”í•œ ë°”ì´íŠ¸ ìˆ˜

    Returns:
        ê³µê°„ì´ ì¶©ë¶„í•˜ë©´ True
    """
    usage = shutil.disk_usage(output_dir)
    if usage.free < required_bytes:
        logger.warning(
            f"Insufficient disk space: {usage.free / (1024**3):.1f}GB available, "
            f"{required_bytes / (1024**3):.1f}GB required"
        )
        return False
    return True


@dataclass
class ValidatedArgs:
    """ê²€ì¦ëœ CLI ì¸ì.

    ``argparse.Namespace`` ë¥¼ íƒ€ì… ì•ˆì „í•˜ê²Œ ë³€í™˜í•œ ë°ì´í„°í´ë˜ìŠ¤.
    :func:`validate_args` ì—ì„œ ìƒì„±ëœë‹¤.
    """

    targets: list[Path]
    output: Path | None
    output_dir: Path | None
    no_resume: bool
    keep_temp: bool
    dry_run: bool
    denoise: bool = False
    denoise_level: str = "medium"
    normalize_audio: bool = False
    group_sequences: bool = True
    fade_duration: float = 0.5
    upload: bool = False
    parallel: int = 1
    thumbnail: bool = False
    thumbnail_timestamps: list[str] | None = None
    thumbnail_quality: int = 2
    set_thumbnail: Path | None = None
    generated_thumbnail_paths: list[Path] | None = None
    detect_silence: bool = False
    trim_silence: bool = False
    silence_threshold: str = "-30dB"
    silence_min_duration: float = 2.0
    bgm_path: Path | None = None
    bgm_volume: float = 0.2
    bgm_loop: bool = False
    exclude_patterns: list[str] | None = None
    include_only_patterns: list[str] | None = None
    sort_key: str = "time"
    reorder: bool = False
    split_duration: str | None = None
    split_size: str | None = None
    archive_originals: Path | None = None
    archive_force: bool = False
    timelapse_speed: int | None = None
    timelapse_audio: bool = False
    timelapse_resolution: str | None = None
    stabilize: bool = False
    stabilize_strength: str = "medium"
    stabilize_crop: str = "crop"
    project: str | None = None
    lut_path: Path | None = None
    auto_lut: bool = False
    lut_before_hdr: bool = False
    device_luts: dict[str, str] | None = None
    notify: bool = False
    schedule: str | None = None
    quality_report: bool = False


@dataclass(frozen=True)
class TranscodeOptions:
    """íŠ¸ëœìŠ¤ì½”ë”© ê³µí†µ ì˜µì…˜.

    ``_transcode_single``, ``_transcode_parallel``, ``_transcode_sequential``
    ì—ì„œ ê³µìœ í•˜ëŠ” ì˜¤ë””ì˜¤Â·í˜ì´ë“œ ì„¤ì •ì„ ë¬¶ëŠ”ë‹¤.

    Attributes:
        denoise: ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ ì œê±° ì—¬ë¶€ (afftdn)
        denoise_level: ë…¸ì´ì¦ˆ ì œê±° ê°•ë„ (``light`` | ``medium`` | ``heavy``)
        normalize_audio: EBU R128 loudnorm 2-pass ì ìš© ì—¬ë¶€
        fade_map: íŒŒì¼ë³„ í˜ì´ë“œ ì„¤ì • ë§µ (ê·¸ë£¹ ê²½ê³„ ê¸°ë°˜)
        fade_duration: ê¸°ë³¸ í˜ì´ë“œ ì‹œê°„ (ì´ˆ)
        trim_silence: ë¬´ìŒ êµ¬ê°„ ì œê±° ì—¬ë¶€
        silence_threshold: ë¬´ìŒ ê¸°ì¤€ ë°ì‹œë²¨
        silence_min_duration: ìµœì†Œ ë¬´ìŒ ê¸¸ì´ (ì´ˆ)
        lut_path: LUT íŒŒì¼ ê²½ë¡œ (ì§ì ‘ ì§€ì •, auto_lutë³´ë‹¤ ìš°ì„ )
        auto_lut: ê¸°ê¸° ëª¨ë¸ ê¸°ë°˜ ìë™ LUT ë§¤ì¹­ í™œì„±í™”
        lut_before_hdr: LUTë¥¼ HDRâ†’SDR ë³€í™˜ ì „ì— ì ìš©
        device_luts: ê¸°ê¸° í‚¤ì›Œë“œ â†’ LUT íŒŒì¼ ê²½ë¡œ ë§¤í•‘
    """

    denoise: bool = False
    denoise_level: str = "medium"
    normalize_audio: bool = False
    fade_map: dict[Path, FadeConfig] | None = None
    fade_duration: float = 0.5
    trim_silence: bool = False
    silence_threshold: str = "-30dB"
    silence_min_duration: float = 2.0
    stabilize: bool = False
    stabilize_strength: str = "medium"
    stabilize_crop: str = "crop"
    lut_path: Path | None = None
    auto_lut: bool = False
    lut_before_hdr: bool = False
    device_luts: dict[str, str] | None = None


@dataclass(frozen=True)
class TranscodeResult:
    """ë‹¨ì¼ íŠ¸ëœìŠ¤ì½”ë”© ê²°ê³¼.

    Attributes:
        output_path: íŠ¸ëœìŠ¤ì½”ë”©ëœ ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        video_id: DB ``videos`` í…Œì´ë¸” ID
        clip_info: í´ë¦½ ë©”íƒ€ë°ì´í„° (íŒŒì¼ëª…, ê¸¸ì´, ê¸°ê¸°ëª…, ì´¬ì˜ì‹œê°)
        silence_segments: ë¬´ìŒ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ (trim_silence í™œì„±í™” ì‹œ)
    """

    output_path: Path
    video_id: int
    clip_info: ClipInfo
    silence_segments: list[SilenceSegment] | None = None


def create_parser() -> argparse.ArgumentParser:
    """
    CLI íŒŒì„œ ìƒì„±.

    Returns:
        argparse.ArgumentParser ì¸ìŠ¤í„´ìŠ¤
    """
    parser = argparse.ArgumentParser(
        prog="tubearchive",
        description=f"ë‹¤ì–‘í•œ ê¸°ê¸°ì˜ 4K ì˜ìƒì„ í‘œì¤€í™”í•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤. (v{__version__})",
        epilog=(
            "ì˜ˆì‹œ:\n"
            "  tubearchive video1.mp4 video2.mov -o merged.mp4  # ë³‘í•©\n"
            "  tubearchive ~/Videos/ --upload                   # ë³‘í•© í›„ ì—…ë¡œë“œ\n"
            "  tubearchive --upload-only merged.mp4             # ì—…ë¡œë“œë§Œ"
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "-V",
        "--version",
        action="version",
        version=f"%(prog)s {__version__}",
    )

    parser.add_argument(
        "targets",
        nargs="*",
        default=[],
        help="ì˜ìƒ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ (ê¸°ë³¸: í˜„ì¬ ë””ë ‰í† ë¦¬)",
    )

    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default=None,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: merged_output.mp4)",
    )

    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="Resume ê¸°ëŠ¥ ë¹„í™œì„±í™”",
    )

    parser.add_argument(
        "--keep-temp",
        action="store_true",
        help="ì„ì‹œ íŒŒì¼ ë³´ì¡´ (ë””ë²„ê¹…ìš©)",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥ (ì‹¤ì œ ì‹¤í–‰ ì•ˆ í•¨)",
    )

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help=f"ì¶œë ¥ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ (í™˜ê²½ë³€ìˆ˜: {ENV_OUTPUT_DIR})",
    )

    # YouTube ì—…ë¡œë“œ ì˜µì…˜
    parser.add_argument(
        "--upload",
        action="store_true",
        help="ë³‘í•© ì™„ë£Œ í›„ YouTubeì— ì—…ë¡œë“œ",
    )

    parser.add_argument(
        "--upload-only",
        type=str,
        metavar="FILE",
        default=None,
        help="ì§€ì •ëœ íŒŒì¼ì„ YouTubeì— ì—…ë¡œë“œ (ë³‘í•© ì—†ì´)",
    )

    parser.add_argument(
        "--upload-title",
        type=str,
        default=None,
        help="YouTube ì—…ë¡œë“œ ì‹œ ì˜ìƒ ì œëª© (ê¸°ë³¸: íŒŒì¼ëª…)",
    )

    parser.add_argument(
        "--upload-privacy",
        type=str,
        default=None,
        choices=["public", "unlisted", "private"],
        help="YouTube ê³µê°œ ì„¤ì • (ê¸°ë³¸: unlisted)",
    )

    parser.add_argument(
        "--schedule",
        type=str,
        default=None,
        metavar="DATETIME",
        help=(
            "YouTube ì˜ˆì•½ ê³µê°œ ì‹œê°„ (ISO 8601 í˜•ì‹, "
            "ì˜ˆ: 2026-02-01T18:00 ë˜ëŠ” 2026-02-01T18:00:00+09:00)"
        ),
    )

    parser.add_argument(
        "--playlist",
        type=str,
        action="append",
        default=None,
        metavar="ID",
        help=(f"ì—…ë¡œë“œ í›„ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (í™˜ê²½ë³€ìˆ˜: {ENV_YOUTUBE_PLAYLIST}, ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    )

    parser.add_argument(
        "--upload-chunk",
        type=int,
        default=None,
        metavar="MB",
        help="ì—…ë¡œë“œ ì²­í¬ í¬ê¸° MB (1-256, í™˜ê²½ë³€ìˆ˜: TUBEARCHIVE_UPLOAD_CHUNK_MB, ê¸°ë³¸: 32)",
    )

    parser.add_argument(
        "--setup-youtube",
        action="store_true",
        help="YouTube ì¸ì¦ ìƒíƒœ í™•ì¸ ë° ì„¤ì • ê°€ì´ë“œ ì¶œë ¥",
    )

    parser.add_argument(
        "--youtube-auth",
        action="store_true",
        help="YouTube ë¸Œë¼ìš°ì € ì¸ì¦ ì‹¤í–‰",
    )

    parser.add_argument(
        "--list-playlists",
        action="store_true",
        help="ë‚´ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ",
    )

    parser.add_argument(
        "--parallel",
        "-j",
        type=int,
        default=None,
        metavar="N",
        help=f"ë³‘ë ¬ íŠ¸ëœìŠ¤ì½”ë”© ìˆ˜ (í™˜ê²½ë³€ìˆ˜: {ENV_PARALLEL}, ê¸°ë³¸: 1)",
    )

    parser.add_argument(
        "--denoise",
        action="store_true",
        help="FFmpeg ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ ì œê±° í™œì„±í™” (afftdn)",
    )

    parser.add_argument(
        "--denoise-level",
        type=str,
        choices=["light", "medium", "heavy"],
        default=None,
        help="ë…¸ì´ì¦ˆ ì œê±° ê°•ë„ (light/medium/heavy, ê¸°ë³¸: medium)",
    )

    parser.add_argument(
        "--normalize-audio",
        action="store_true",
        help="EBU R128 ì˜¤ë””ì˜¤ ë¼ìš°ë“œë‹ˆìŠ¤ ì •ê·œí™” í™œì„±í™” (loudnorm 2-pass)",
    )

    parser.add_argument(
        "--bgm",
        type=str,
        default=None,
        metavar="PATH",
        help="ë°°ê²½ìŒì•… íŒŒì¼ ê²½ë¡œ (MP3, AAC, WAV ë“±)",
    )

    parser.add_argument(
        "--bgm-volume",
        type=float,
        default=None,
        metavar="0.0-1.0",
        help="ë°°ê²½ìŒì•… ìƒëŒ€ ë³¼ë¥¨ (0.0~1.0, ê¸°ë³¸: 0.2)",
    )

    parser.add_argument(
        "--bgm-loop",
        action="store_true",
        help="BGM ê¸¸ì´ < ì˜ìƒ ê¸¸ì´ì¼ ë•Œ ë£¨í”„ ì¬ìƒ",
    )

    parser.add_argument(
        "--detect-silence",
        action="store_true",
        help="ë¬´ìŒ êµ¬ê°„ ê°ì§€ ë° ëª©ë¡ ì¶œë ¥ (ì œê±°í•˜ì§€ ì•ŠìŒ)",
    )

    parser.add_argument(
        "--trim-silence",
        action="store_true",
        help="ì‹œì‘/ë ë¬´ìŒ ìë™ ì œê±°",
    )

    parser.add_argument(
        "--silence-threshold",
        type=str,
        default="-30dB",
        metavar="DB",
        help="ë¬´ìŒ ê¸°ì¤€ dB (ê¸°ë³¸: -30dB)",
    )

    parser.add_argument(
        "--silence-duration",
        type=float,
        default=2.0,
        metavar="SECONDS",
        help="ìµœì†Œ ë¬´ìŒ ê¸¸ì´(ì´ˆ, ê¸°ë³¸: 2.0)",
    )

    parser.add_argument(
        "--stabilize",
        action="store_true",
        help="ì˜ìƒ ì•ˆì •í™” í™œì„±í™” (vidstab 2-pass, íŠ¸ëœìŠ¤ì½”ë”© ì‹œê°„ ì¦ê°€)",
    )

    parser.add_argument(
        "--stabilize-strength",
        type=str,
        choices=["light", "medium", "heavy"],
        default=None,
        help="ì˜ìƒ ì•ˆì •í™” ê°•ë„ (light/medium/heavy, ê¸°ë³¸: medium)",
    )

    parser.add_argument(
        "--stabilize-crop",
        type=str,
        choices=["crop", "expand"],
        default=None,
        help="ì•ˆì •í™” í›„ í”„ë ˆì„ ì²˜ë¦¬ (crop: ì˜ë¼ëƒ„, expand: ê²€ì€ìƒ‰ ì±„ì›€, ê¸°ë³¸: crop)",
    )

    group_toggle = parser.add_mutually_exclusive_group()
    group_toggle.add_argument(
        "--group",
        action="store_true",
        help=f"ì—°ì† íŒŒì¼ ì‹œí€€ìŠ¤ ê·¸ë£¹í•‘ í™œì„±í™” (í™˜ê²½ë³€ìˆ˜: {ENV_GROUP_SEQUENCES})",
    )
    group_toggle.add_argument(
        "--no-group",
        action="store_true",
        help="ì—°ì† íŒŒì¼ ì‹œí€€ìŠ¤ ê·¸ë£¹í•‘ ë¹„í™œì„±í™”",
    )

    parser.add_argument(
        "--fade-duration",
        type=float,
        default=None,
        metavar="SECONDS",
        help=f"ê¸°ë³¸ í˜ì´ë“œ ì‹œê°„(ì´ˆ) ì„¤ì • (í™˜ê²½ë³€ìˆ˜: {ENV_FADE_DURATION}, ê¸°ë³¸: 0.5)",
    )

    parser.add_argument(
        "--exclude",
        type=str,
        action="append",
        default=None,
        metavar="PATTERN",
        help="ì œì™¸í•  íŒŒì¼ëª… íŒ¨í„´ (ê¸€ë¡œë¸Œ, ë°˜ë³µ ê°€ëŠ¥, ì˜ˆ: 'GH*' '*.mts')",
    )

    parser.add_argument(
        "--include-only",
        type=str,
        action="append",
        default=None,
        metavar="PATTERN",
        help="í¬í•¨í•  íŒŒì¼ëª… íŒ¨í„´ë§Œ ì„ íƒ (ê¸€ë¡œë¸Œ, ë°˜ë³µ ê°€ëŠ¥, ì˜ˆ: '*.mp4')",
    )

    parser.add_argument(
        "--sort",
        type=str,
        default=None,
        choices=[k.value for k in SortKey],
        help="ì •ë ¬ ê¸°ì¤€ ë³€ê²½ (ê¸°ë³¸: time, ì˜µì…˜: name/size/device)",
    )

    parser.add_argument(
        "--reorder",
        action="store_true",
        help="ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œë¡œ í´ë¦½ ìˆœì„œ ìˆ˜ë™ í¸ì§‘",
    )

    parser.add_argument(
        "--config",
        type=str,
        default=None,
        metavar="PATH",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: ~/.tubearchive/config.toml)",
    )

    parser.add_argument(
        "--init-config",
        action="store_true",
        help="ê¸°ë³¸ ì„¤ì • íŒŒì¼(config.toml) ìƒì„±",
    )

    parser.add_argument(
        "--reset-build",
        type=str,
        nargs="?",
        const="",
        metavar="PATH",
        help="íŠ¸ëœìŠ¤ì½”ë”©/ë³‘í•© ê¸°ë¡ ì´ˆê¸°í™” (ë‹¤ì‹œ ë¹Œë“œ, ê²½ë¡œ ì§€ì • ë˜ëŠ” ëª©ë¡ì—ì„œ ì„ íƒ)",
    )

    parser.add_argument(
        "--reset-upload",
        type=str,
        nargs="?",
        const="",
        metavar="PATH",
        help="YouTube ì—…ë¡œë“œ ê¸°ë¡ ì´ˆê¸°í™” (ë‹¤ì‹œ ì—…ë¡œë“œ, ê²½ë¡œ ì§€ì • ë˜ëŠ” ëª©ë¡ì—ì„œ ì„ íƒ)",
    )

    # ì¸ë„¤ì¼ ì˜µì…˜
    parser.add_argument(
        "--thumbnail",
        action="store_true",
        help="ë³‘í•© ì˜ìƒì—ì„œ ì¸ë„¤ì¼ ìë™ ìƒì„± (ê¸°ë³¸: 10%%, 33%%, 50%% ì§€ì )",
    )

    parser.add_argument(
        "--thumbnail-at",
        type=str,
        action="append",
        default=None,
        metavar="TIMESTAMP",
        help="íŠ¹ì • ì‹œì ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ (ì˜ˆ: '00:01:30', ë°˜ë³µ ê°€ëŠ¥)",
    )

    parser.add_argument(
        "--thumbnail-quality",
        type=int,
        default=2,
        metavar="Q",
        help="ì¸ë„¤ì¼ JPEG í’ˆì§ˆ (1-31, ë‚®ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ, ê¸°ë³¸: 2)",
    )

    parser.add_argument(
        "--set-thumbnail",
        type=str,
        default=None,
        metavar="PATH",
        help="YouTube ì—…ë¡œë“œ ì‹œ ì‚¬ìš©í•  ì¸ë„¤ì¼ ì´ë¯¸ì§€ ê²½ë¡œ (JPG/PNG)",
    )

    parser.add_argument(
        "--quality-report",
        action="store_true",
        help="íŠ¸ëœìŠ¤ì½”ë”© ê²°ê³¼ SSIM/PSNR/VMAF ì§€í‘œ ì¶œë ¥ (ê°€ëŠ¥í•œ í•„í„°ë§Œ ê³„ì‚°)",
    )

    # ì˜ìƒ ë¶„í•  ì˜µì…˜
    parser.add_argument(
        "--split-duration",
        type=str,
        default=None,
        metavar="DURATION",
        help="ì‹œê°„ ê¸°ì¤€ ë¶„í•  (ì˜ˆ: 1h, 30m, 1h30m), YouTube 12ì‹œê°„ ì œí•œ ëŒ€ì‘",
    )

    parser.add_argument(
        "--split-size",
        type=str,
        default=None,
        metavar="SIZE",
        help="íŒŒì¼ í¬ê¸° ê¸°ì¤€ ë¶„í•  (ì˜ˆ: 10G, 256M), YouTube 256GB ì œí•œ ëŒ€ì‘",
    )

    # íƒ€ì„ë©ìŠ¤ ì˜µì…˜
    parser.add_argument(
        "--timelapse",
        type=str,
        default=None,
        metavar="SPEED",
        help="íƒ€ì„ë©ìŠ¤ ë°°ì† (ì˜ˆ: 10x, ë²”ìœ„: 2x-60x)",
    )

    parser.add_argument(
        "--timelapse-audio",
        action="store_true",
        help="íƒ€ì„ë©ìŠ¤ì—ì„œ ì˜¤ë””ì˜¤ ê°€ì† (ê¸°ë³¸: ì˜¤ë””ì˜¤ ì œê±°)",
    )

    parser.add_argument(
        "--timelapse-resolution",
        type=str,
        default=None,
        metavar="RES",
        help="íƒ€ì„ë©ìŠ¤ ì¶œë ¥ í•´ìƒë„ (ì˜ˆ: 1080p, 4k, 1920x1080, ê¸°ë³¸: ì›ë³¸ ìœ ì§€)",
    )

    # LUT ì»¬ëŸ¬ ê·¸ë ˆì´ë”© ì˜µì…˜
    parser.add_argument(
        "--lut",
        type=str,
        default=None,
        metavar="PATH",
        help="LUT íŒŒì¼ ê²½ë¡œ (.cube, .3dl) â€” íŠ¸ëœìŠ¤ì½”ë”© ì‹œ lut3d í•„í„° ì ìš©",
    )

    # default=Noneìœ¼ë¡œ "CLIì—ì„œ ëª…ì‹œí•˜ì§€ ì•ŠìŒ"ê³¼ "ëª…ì‹œì  True"ë¥¼ êµ¬ë¶„.
    # Noneì´ë©´ í™˜ê²½ë³€ìˆ˜/config ê¸°ë³¸ê°’(get_default_auto_lut())ìœ¼ë¡œ ê²°ì •.
    parser.add_argument(
        "--auto-lut",
        action="store_true",
        default=None,
        help="ê¸°ê¸° ëª¨ë¸ ê¸°ë°˜ ìë™ LUT ë§¤ì¹­ (config.toml [color_grading.device_luts] ì°¸ì¡°)",
    )

    parser.add_argument(
        "--no-auto-lut",
        action="store_true",
        help="ìë™ LUT ë§¤ì¹­ ë¹„í™œì„±í™” (í™˜ê²½ë³€ìˆ˜/config ì„¤ì • ë¬´ì‹œ)",
    )

    parser.add_argument(
        "--lut-before-hdr",
        action="store_true",
        help="LUTë¥¼ HDRâ†’SDR ë³€í™˜ ì „ì— ì ìš© (ê¸°ë³¸: HDR ë³€í™˜ í›„ ì ìš©)",
    )

    parser.add_argument(
        "--status",
        nargs="?",
        const=CATALOG_STATUS_SENTINEL,
        default=None,
        metavar="STATUS",
        help=(
            "ì‘ì—… í˜„í™© ì¡°íšŒ (ê°’ ì§€ì • ì‹œ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ìƒíƒœ í•„í„°ë¡œ ì‚¬ìš©: "
            "pending/processing/completed/failed/merged/untracked)"
        ),
    )

    parser.add_argument(
        "--status-detail",
        type=int,
        metavar="ID",
        default=None,
        help="íŠ¹ì • ì‘ì—… ìƒì„¸ ì¡°íšŒ (merge_job ID)",
    )

    parser.add_argument(
        "--stats",
        action="store_true",
        help="ì „ì²´ ì²˜ë¦¬ í†µê³„ ëŒ€ì‹œë³´ë“œ ì¡°íšŒ",
    )

    parser.add_argument(
        "--period",
        type=str,
        default=None,
        metavar="PERIOD",
        help="í†µê³„ ê¸°ê°„ í•„í„° (ì˜ˆ: 2026-01, 2026). --statsì™€ í•¨ê»˜ ì‚¬ìš©",
    )

    parser.add_argument(
        "--catalog",
        action="store_true",
        help="ì˜ìƒ ë©”íƒ€ë°ì´í„° ì „ì²´ ëª©ë¡ ì¡°íšŒ (ê¸°ê¸°ë³„ ê·¸ë£¹í•‘)",
    )

    parser.add_argument(
        "--search",
        nargs="?",
        const="",
        default=None,
        metavar="PATTERN",
        help="ì˜ìƒ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (ì˜ˆ: 2026-01)",
    )

    parser.add_argument(
        "--device",
        type=str,
        default=None,
        metavar="NAME",
        help="ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì‹œ ê¸°ê¸° í•„í„° (ì˜ˆ: GoPro)",
    )

    # í”„ë¡œì íŠ¸ ì˜µì…˜
    parser.add_argument(
        "--project",
        type=str,
        default=None,
        metavar="NAME",
        help='í”„ë¡œì íŠ¸ì— ë³‘í•© ê²°ê³¼ ì—°ê²° (ì—†ìœ¼ë©´ ìë™ ìƒì„±, ì˜ˆ: "ì œì£¼ë„ ì—¬í–‰")',
    )

    parser.add_argument(
        "--project-list",
        action="store_true",
        help="í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ (--json ì˜µì…˜ìœ¼ë¡œ JSON ì¶œë ¥)",
    )

    parser.add_argument(
        "--project-detail",
        type=int,
        default=None,
        metavar="ID",
        help="í”„ë¡œì íŠ¸ ìƒì„¸ ì¡°íšŒ (í”„ë¡œì íŠ¸ ID, --json ì˜µì…˜ìœ¼ë¡œ JSON ì¶œë ¥)",
    )

    # ì•„ì¹´ì´ë¸Œ ì˜µì…˜
    parser.add_argument(
        "--archive-originals",
        type=str,
        default=None,
        metavar="PATH",
        help="íŠ¸ëœìŠ¤ì½”ë”© ì™„ë£Œ í›„ ì›ë³¸ íŒŒì¼ì„ ì§€ì • ê²½ë¡œë¡œ ì´ë™",
    )

    parser.add_argument(
        "--archive-force",
        action="store_true",
        help="ì›ë³¸ íŒŒì¼ ì‚­ì œ(delete ì •ì±…) ì‹œ í™•ì¸ í”„ë¡¬í”„íŠ¸ ìš°íšŒ",
    )

    # ì•Œë¦¼ ì˜µì…˜
    parser.add_argument(
        "--notify",
        action="store_true",
        help="íŒŒì´í”„ë¼ì¸ ì™„ë£Œ/ì—ëŸ¬ ì‹œ ì•Œë¦¼ ì „ì†¡ (config.toml [notification] ì„¤ì • í•„ìš”)",
    )
    parser.add_argument(
        "--notify-test",
        action="store_true",
        help="ì„¤ì •ëœ ì•Œë¦¼ ì±„ë„ì— í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ì „ì†¡ í›„ ì¢…ë£Œ",
    )

    output_format_group = parser.add_mutually_exclusive_group()
    output_format_group.add_argument(
        "--json",
        action="store_true",
        help="ë©”íƒ€ë°ì´í„° ì¶œë ¥ í˜•ì‹ì„ JSONìœ¼ë¡œ ì§€ì •",
    )
    output_format_group.add_argument(
        "--csv",
        action="store_true",
        help="ë©”íƒ€ë°ì´í„° ì¶œë ¥ í˜•ì‹ì„ CSVë¡œ ì§€ì •",
    )

    return parser


def parse_schedule_datetime(schedule_str: str) -> str:
    """ISO 8601 í˜•ì‹ì˜ ë‚ ì§œ/ì‹œê°„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ê³  ê²€ì¦í•œë‹¤.

    ê³µë°± êµ¬ë¶„ í˜•ì‹(``2026-02-01 18:00``)ì€ ìë™ìœ¼ë¡œ Të¡œ ë³€í™˜ëœë‹¤.
    íƒ€ì„ì¡´ì´ ì—†ìœ¼ë©´ ë¡œì»¬ íƒ€ì„ì¡´ì´ ìë™ìœ¼ë¡œ ì¶”ê°€ëœë‹¤.

    Args:
        schedule_str: ISO 8601 í˜•ì‹ ë‚ ì§œ/ì‹œê°„ ë¬¸ìì—´

    Returns:
        YouTube APIê°€ ìš”êµ¬í•˜ëŠ” RFC 3339 í˜•ì‹ ë¬¸ìì—´

    Raises:
        ValueError: í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ê³¼ê±° ì‹œê°„ì¼ ë•Œ
    """
    # ê³µë°± êµ¬ë¶„ í˜•ì‹ì„ T êµ¬ë¶„ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: "2026-02-01 18:00" â†’ "2026-02-01T18:00")
    normalized = schedule_str.strip()
    if " " in normalized and "T" not in normalized:
        normalized = normalized.replace(" ", "T", 1)

    try:
        # Python 3.11+ëŠ” fromisoformatì´ ëŒ€ë¶€ë¶„ ISO 8601 í˜•ì‹ ì§€ì›
        parsed_dt = datetime.fromisoformat(normalized)
    except ValueError as e:
        raise ValueError(
            f"Invalid datetime format: {schedule_str}. "
            "Expected ISO 8601 format (e.g., 2026-02-01T18:00, "
            "2026-02-01 18:00, or 2026-02-01T18:00:00+09:00)"
        ) from e

    # íƒ€ì„ì¡´ ì—†ìœ¼ë©´ ë¡œì»¬ íƒ€ì„ì¡´ ìë™ ì¶”ê°€
    if parsed_dt.tzinfo is None:
        try:
            # ì‹œìŠ¤í…œ ë¡œì»¬ íƒ€ì„ì¡´ ê°€ì ¸ì˜¤ê¸°
            local_tz = datetime.now().astimezone().tzinfo
            if local_tz is not None:
                parsed_dt = parsed_dt.replace(tzinfo=local_tz)
                tz_name = local_tz.tzname(parsed_dt) or "local"
                logger.info(f"Local timezone automatically added: {tz_name}")
        except Exception:
            # íƒ€ì„ì¡´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ì¶œë ¥
            logger.warning(
                "Could not determine local timezone. "
                "YouTube will interpret the time as UTC. "
                "Consider specifying timezone explicitly (e.g., +09:00)."
            )

    # ê³¼ê±° ì‹œê°„ ê²€ì¦
    now = datetime.now(parsed_dt.tzinfo)
    if parsed_dt < now:
        # ì–¼ë§ˆë‚˜ ê³¼ê±°ì¸ì§€ ê³„ì‚°
        time_diff = now - parsed_dt
        hours_ago = time_diff.total_seconds() / 3600

        if hours_ago < 1:
            time_desc = f"{int(time_diff.total_seconds() / 60)}ë¶„ ì „"
        elif hours_ago < 24:
            time_desc = f"{int(hours_ago)}ì‹œê°„ ì „"
        else:
            time_desc = f"{int(hours_ago / 24)}ì¼ ì „"

        raise ValueError(
            f"Schedule time must be in the future. "
            f"Specified time is {time_desc}. "
            f"Current time: {now.strftime('%Y-%m-%d %H:%M:%S %Z')}"
        )

    # YouTube APIëŠ” RFC 3339 í˜•ì‹ ìš”êµ¬ (ISO 8601ì˜ ì—„ê²©í•œ ì„œë¸Œì…‹)
    # isoformat()ì´ RFC 3339 í˜¸í™˜ í˜•ì‹ ë°˜í™˜
    return parsed_dt.isoformat()


def _resolve_set_thumbnail_path(
    set_thumbnail_arg: str | Path | None,
) -> Path | None:
    """`--set-thumbnail` ì…ë ¥ê°’ì„ ì •ê·œí™”í•˜ê³  ê²€ì¦í•œë‹¤.

    - ê²½ë¡œ í™•ì¥: `~` ì „ê°œ + `resolve()`
    - ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
    - í¬ë§· ê²€ì¦ (`.jpg`, `.jpeg`, `.png`)

    Args:
        set_thumbnail_arg: CLI ì…ë ¥ê°’(`--set-thumbnail`)

    Returns:
        ê²€ì¦ëœ Path ë˜ëŠ” ë¯¸ì§€ì • ì‹œ None
    """
    if not set_thumbnail_arg:
        return None

    set_thumbnail = Path(set_thumbnail_arg).expanduser().resolve()
    if not set_thumbnail.is_file():
        raise FileNotFoundError(f"Thumbnail file not found: {set_thumbnail_arg}")

    if set_thumbnail.suffix.lower() not in SUPPORTED_THUMBNAIL_EXTENSIONS:
        raise ValueError(
            f"Unsupported thumbnail format: {set_thumbnail.suffix} (supported: .jpg, .jpeg, .png)"
        )

    return set_thumbnail


def validate_args(
    args: argparse.Namespace,
    device_luts: dict[str, str] | None = None,
) -> ValidatedArgs:
    """CLI ì¸ìë¥¼ ê²€ì¦í•˜ê³  :class:`ValidatedArgs` ë¡œ ë³€í™˜í•œë‹¤.

    ê° ì„¤ì •ì˜ ìš°ì„ ìˆœìœ„: **CLI ì˜µì…˜ > í™˜ê²½ë³€ìˆ˜ > config.toml > ê¸°ë³¸ê°’**.
    ``get_default_*()`` í—¬í¼ê°€ í™˜ê²½ë³€ìˆ˜Â·config.tomlì„ ì´ë¯¸ ë°˜ì˜í•˜ë¯€ë¡œ,
    ì—¬ê¸°ì„œëŠ” CLI ì¸ìê°€ ëª…ì‹œë˜ì—ˆëŠ”ì§€ë§Œ í™•ì¸í•œë‹¤.

    Args:
        args: ``argparse`` íŒŒì‹± ê²°ê³¼

    Returns:
        íƒ€ì…-ì•ˆì „í•˜ê²Œ ê²€ì¦ëœ ì¸ì ë°ì´í„°í´ë˜ìŠ¤

    Raises:
        FileNotFoundError: ëŒ€ìƒ íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì—†ì„ ë•Œ
        ValueError: fade_duration < 0 ë˜ëŠ” thumbnail_quality ë²”ìœ„ ì´ˆê³¼
    """
    # targets ê²€ì¦
    targets: list[Path] = []
    if not args.targets:
        targets = [Path.cwd()]
    else:
        for target in args.targets:
            path = Path(target)
            if not path.exists():
                raise FileNotFoundError(f"Target not found: {target}")
            targets.append(path)

    # output ê²€ì¦
    output: Path | None = None
    if args.output:
        output = Path(args.output)
        if not output.parent.exists():
            raise FileNotFoundError(f"Output directory not found: {output.parent}")

    # output_dir ê²€ì¦ (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > None)
    output_dir: Path | None = None
    if args.output_dir:
        output_dir = Path(args.output_dir)
        if not output_dir.is_dir():
            raise FileNotFoundError(f"Output directory not found: {args.output_dir}")
    else:
        output_dir = get_default_output_dir()

    # upload í”Œë˜ê·¸ í™•ì¸
    upload = getattr(args, "upload", False)

    # parallel ê°’ ê²°ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    parallel = args.parallel if args.parallel is not None else get_default_parallel()
    if parallel < 1:
        parallel = 1

    # denoise ì„¤ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    denoise_flag = bool(getattr(args, "denoise", False))
    denoise_level = getattr(args, "denoise_level", None)
    env_denoise = get_default_denoise()
    env_denoise_level = get_default_denoise_level()
    if denoise_level is not None:
        denoise_flag = True
    resolved_denoise_level = denoise_level or env_denoise_level or "medium"
    if env_denoise_level is not None or env_denoise:
        denoise_flag = True

    # normalize_audio ì„¤ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    normalize_audio = bool(getattr(args, "normalize_audio", False)) or get_default_normalize_audio()

    # ê·¸ë£¹í•‘ ì„¤ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    group_flag = bool(getattr(args, "group", False))
    no_group_flag = bool(getattr(args, "no_group", False))
    if group_flag:
        group_sequences = True
    elif no_group_flag:
        group_sequences = False
    else:
        group_sequences = get_default_group_sequences()

    # fade_duration ì„¤ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    fade_duration_arg = getattr(args, "fade_duration", None)
    fade_duration = (
        fade_duration_arg if fade_duration_arg is not None else get_default_fade_duration()
    )
    if fade_duration < 0:
        raise ValueError(f"Fade duration must be >= 0, got: {fade_duration}")

    # ì¸ë„¤ì¼ ì˜µì…˜ ê²€ì¦
    thumbnail = getattr(args, "thumbnail", False)
    thumbnail_at: list[str] | None = getattr(args, "thumbnail_at", None)
    thumbnail_quality: int = getattr(args, "thumbnail_quality", 2)
    set_thumbnail_arg = getattr(args, "set_thumbnail", None)
    set_thumbnail = _resolve_set_thumbnail_path(set_thumbnail_arg)

    # --thumbnail-atë§Œ ì§€ì •í•´ë„ ì•”ë¬µì  í™œì„±í™”
    if thumbnail_at and not thumbnail:
        thumbnail = True

    # quality ë²”ìœ„ ê²€ì¦
    if not 1 <= thumbnail_quality <= 31:
        raise ValueError(f"Thumbnail quality must be 1-31, got: {thumbnail_quality}")

    # í™”ì§ˆ ë¦¬í¬íŠ¸ ì˜µì…˜
    quality_report = bool(getattr(args, "quality_report", False))

    # ë¬´ìŒ ê´€ë ¨ ì˜µì…˜
    detect_silence = getattr(args, "detect_silence", False)
    trim_silence = getattr(args, "trim_silence", False)
    silence_threshold = getattr(args, "silence_threshold", "-30dB")
    silence_min_duration = getattr(args, "silence_duration", 2.0)

    # silence_min_duration ë²”ìœ„ ê²€ì¦
    if silence_min_duration <= 0:
        raise ValueError(f"Silence duration must be > 0, got: {silence_min_duration}")

    # BGM ì˜µì…˜ ê²€ì¦ (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    bgm_path_arg = getattr(args, "bgm", None)
    bgm_path: Path | None = None
    if bgm_path_arg:
        bgm_path = Path(bgm_path_arg).expanduser()
        if not bgm_path.is_file():
            raise FileNotFoundError(f"BGM file not found: {bgm_path_arg}")
    else:
        # í™˜ê²½ë³€ìˆ˜/ì„¤ì •íŒŒì¼ì—ì„œ ê¸°ë³¸ê°’
        bgm_path = get_default_bgm_path()

    bgm_volume_arg = getattr(args, "bgm_volume", None)
    if bgm_volume_arg is not None:
        if not (0.0 <= bgm_volume_arg <= 1.0):
            raise ValueError(f"BGM volume must be in range [0.0, 1.0], got: {bgm_volume_arg}")
        bgm_volume = bgm_volume_arg
    else:
        # í™˜ê²½ë³€ìˆ˜/ì„¤ì •íŒŒì¼ì—ì„œ ê¸°ë³¸ê°’, ì—†ìœ¼ë©´ 0.2
        env_bgm_volume = get_default_bgm_volume()
        bgm_volume = env_bgm_volume if env_bgm_volume is not None else 0.2

    bgm_loop_arg = getattr(args, "bgm_loop", False)
    bgm_loop = bgm_loop_arg or get_default_bgm_loop()

    # stabilize ì„¤ì • (CLI ì¸ì > í™˜ê²½ ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    # --stabilize-strength ë˜ëŠ” --stabilize-crop ì§€ì • ì‹œ ì•”ë¬µì ìœ¼ë¡œ í™œì„±í™”
    stabilize_strength_arg: str | None = getattr(args, "stabilize_strength", None)
    stabilize_crop_arg: str | None = getattr(args, "stabilize_crop", None)
    env_stabilize = get_default_stabilize()
    env_stabilize_strength = get_default_stabilize_strength()
    env_stabilize_crop = get_default_stabilize_crop()
    stabilize_flag = (
        bool(getattr(args, "stabilize", False))
        or stabilize_strength_arg is not None
        or stabilize_crop_arg is not None
        or env_stabilize
    )
    resolved_stabilize_strength = stabilize_strength_arg or env_stabilize_strength or "medium"
    resolved_stabilize_crop = stabilize_crop_arg or env_stabilize_crop or "crop"

    exclude_patterns: list[str] | None = getattr(args, "exclude", None)
    include_only_patterns: list[str] | None = getattr(args, "include_only", None)
    sort_key_str: str = getattr(args, "sort", None) or "time"
    reorder_flag: bool = getattr(args, "reorder", False)

    # ì˜ìƒ ë¶„í•  ì˜µì…˜
    split_duration: str | None = getattr(args, "split_duration", None)
    split_size: str | None = getattr(args, "split_size", None)

    # ì•„ì¹´ì´ë¸Œ ì˜µì…˜
    archive_originals_arg = getattr(args, "archive_originals", None)
    archive_originals: Path | None = None
    if archive_originals_arg:
        archive_originals = Path(archive_originals_arg).expanduser().resolve()
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„± ì˜ˆì •ì´ë¯€ë¡œ ê²€ì¦ ìƒëµ

    archive_force_flag: bool = getattr(args, "archive_force", False)

    # íƒ€ì„ë©ìŠ¤ ì˜µì…˜ ê²€ì¦
    timelapse_speed: int | None = None
    if hasattr(args, "timelapse") and args.timelapse:
        timelapse_str = args.timelapse.lower().rstrip("x")
        try:
            timelapse_speed = int(timelapse_str)
        except ValueError:
            raise ValueError(f"Invalid timelapse speed format: {args.timelapse}") from None

        from tubearchive.ffmpeg.effects import TIMELAPSE_MAX_SPEED, TIMELAPSE_MIN_SPEED

        if timelapse_speed < TIMELAPSE_MIN_SPEED or timelapse_speed > TIMELAPSE_MAX_SPEED:
            raise ValueError(
                f"Timelapse speed must be between {TIMELAPSE_MIN_SPEED}x and "
                f"{TIMELAPSE_MAX_SPEED}x, got {timelapse_speed}x"
            )

    timelapse_audio: bool = getattr(args, "timelapse_audio", False)
    timelapse_resolution: str | None = getattr(args, "timelapse_resolution", None)

    # LUT ì˜µì…˜ ê²€ì¦ (CLI ì¸ì > í™˜ê²½ë³€ìˆ˜ > config > ê¸°ë³¸ê°’)
    lut_path_arg = getattr(args, "lut", None)
    lut_path: Path | None = None
    if lut_path_arg:
        lut_path = Path(lut_path_arg).expanduser().resolve()
        if not lut_path.is_file():
            raise FileNotFoundError(f"LUT file not found: {lut_path_arg}")
        ext = lut_path.suffix.lower()
        if ext not in LUT_SUPPORTED_EXTENSIONS:
            raise ValueError(
                f"Unsupported LUT format: {ext} "
                f"(supported: {', '.join(sorted(LUT_SUPPORTED_EXTENSIONS))})"
            )

    auto_lut_flag = getattr(args, "auto_lut", None)
    no_auto_lut_flag = getattr(args, "no_auto_lut", False)
    # --no-auto-lutì´ --auto-lutë³´ë‹¤ ìš°ì„  (ëª…ì‹œì  ë¹„í™œì„±í™”)
    if no_auto_lut_flag:
        if auto_lut_flag:
            logger.warning("--auto-lut and --no-auto-lut both set; --no-auto-lut wins")
        auto_lut = False
    elif auto_lut_flag:
        auto_lut = True
    else:
        auto_lut = get_default_auto_lut()

    lut_before_hdr: bool = getattr(args, "lut_before_hdr", False)

    # ìŠ¤ì¼€ì¤„ ì˜µì…˜ ê²€ì¦
    schedule_arg: str | None = getattr(args, "schedule", None)
    schedule: str | None = None
    if schedule_arg:
        schedule = parse_schedule_datetime(schedule_arg)
        logger.info(f"Parsed schedule time: {schedule}")

    return ValidatedArgs(
        targets=targets,
        output=output,
        output_dir=output_dir,
        no_resume=args.no_resume,
        keep_temp=args.keep_temp,
        dry_run=args.dry_run,
        denoise=denoise_flag,
        denoise_level=resolved_denoise_level,
        normalize_audio=normalize_audio,
        group_sequences=group_sequences,
        fade_duration=fade_duration,
        upload=upload,
        parallel=parallel,
        thumbnail=thumbnail,
        thumbnail_timestamps=thumbnail_at,
        thumbnail_quality=thumbnail_quality,
        set_thumbnail=set_thumbnail,
        generated_thumbnail_paths=None,
        detect_silence=detect_silence,
        trim_silence=trim_silence,
        silence_threshold=silence_threshold,
        silence_min_duration=silence_min_duration,
        bgm_path=bgm_path,
        bgm_volume=bgm_volume,
        bgm_loop=bgm_loop,
        exclude_patterns=exclude_patterns,
        include_only_patterns=include_only_patterns,
        sort_key=sort_key_str,
        reorder=reorder_flag,
        split_duration=split_duration,
        split_size=split_size,
        archive_originals=archive_originals,
        archive_force=archive_force_flag,
        timelapse_speed=timelapse_speed,
        timelapse_audio=timelapse_audio,
        timelapse_resolution=timelapse_resolution,
        stabilize=stabilize_flag,
        stabilize_strength=resolved_stabilize_strength,
        stabilize_crop=resolved_stabilize_crop,
        project=getattr(args, "project", None),
        lut_path=lut_path,
        auto_lut=auto_lut,
        lut_before_hdr=lut_before_hdr,
        device_luts=device_luts if device_luts else None,
        quality_report=quality_report,
        notify=bool(getattr(args, "notify", False)) or get_default_notify(),
        schedule=schedule,
    )


def setup_logging(verbose: bool = False) -> None:
    """
    ë¡œê¹… ì„¤ì •.

    Args:
        verbose: ìƒì„¸ ë¡œê·¸ ì—¬ë¶€
    """
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def get_output_filename(targets: list[Path]) -> str:
    """
    ì…ë ¥ íƒ€ê²Ÿì—ì„œ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±.

    ë””ë ‰í† ë¦¬ëª… ë˜ëŠ” ì²« ë²ˆì§¸ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ëª…ì„ ì‚¬ìš©.

    Args:
        targets: ì…ë ¥ íƒ€ê²Ÿ ëª©ë¡

    Returns:
        ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
    """
    if not targets:
        return "output.mp4"

    first_target = targets[0]
    name = first_target.name if first_target.is_dir() else first_target.parent.name

    # ë¹ˆ ì´ë¦„ì´ê±°ë‚˜ í˜„ì¬ ë””ë ‰í† ë¦¬ë©´ ê¸°ë³¸ê°’
    if not name or name == ".":
        name = "output"

    return f"{name}.mp4"


def _get_media_duration(media_path: Path) -> float:
    """ffprobeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë””ì–´ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        media_path: ë¯¸ë””ì–´ íŒŒì¼ ê²½ë¡œ

    Returns:
        ê¸¸ì´ (ì´ˆ)

    Raises:
        RuntimeError: ffprobe ì‹¤í–‰ ì‹¤íŒ¨ ë˜ëŠ” ê¸¸ì´ íŒŒì‹± ì‹¤íŒ¨
    """
    try:
        probe_result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "quiet",
                "-print_format",
                "json",
                "-show_format",
                str(media_path),
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        info = json.loads(probe_result.stdout)
        return float(info["format"]["duration"])
    except (subprocess.CalledProcessError, KeyError, ValueError) as e:
        raise RuntimeError(f"Failed to probe duration: {media_path} - {e}") from e


def _has_audio_stream(media_path: Path) -> bool:
    """ffprobeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë””ì–´ íŒŒì¼ì— ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.

    Args:
        media_path: ë¯¸ë””ì–´ íŒŒì¼ ê²½ë¡œ

    Returns:
        ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¡´ì¬ ì—¬ë¶€
    """
    try:
        probe_result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "quiet",
                "-print_format",
                "json",
                "-show_streams",
                "-select_streams",
                "a",
                str(media_path),
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        info = json.loads(probe_result.stdout)
        streams = info.get("streams", [])
        return len(streams) > 0
    except (subprocess.CalledProcessError, ValueError):
        return False


def _apply_bgm_mixing(
    video_path: Path,
    bgm_path: Path,
    bgm_volume: float,
    bgm_loop: bool,
    output_path: Path,
) -> Path:
    """ë³‘í•©ëœ ì˜ìƒì— BGMì„ ë¯¹ì‹±í•œë‹¤.

    ffprobeë¡œ ì˜ìƒ/BGM ê¸¸ì´ì™€ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•œ ë’¤
    :func:`~tubearchive.ffmpeg.effects.create_bgm_filter` ë¡œ í•„í„°ë¥¼ ìƒì„±í•˜ê³ 
    ffmpegë¡œ ì˜¤ë””ì˜¤ë§Œ ì¬ì¸ì½”ë”©í•œë‹¤ (ì˜ìƒì€ ``-c:v copy``).

    Args:
        video_path: ë³‘í•©ëœ ì˜ìƒ íŒŒì¼ ê²½ë¡œ
        bgm_path: BGM íŒŒì¼ ê²½ë¡œ
        bgm_volume: BGM ìƒëŒ€ ë³¼ë¥¨ (0.0~1.0)
        bgm_loop: BGM ë£¨í”„ ì¬ìƒ ì—¬ë¶€
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ

    Returns:
        BGMì´ ë¯¹ì‹±ëœ ìµœì¢… íŒŒì¼ ê²½ë¡œ

    Raises:
        RuntimeError: FFmpeg ì‹¤í–‰ ì‹¤íŒ¨
    """
    from tubearchive.ffmpeg.effects import create_bgm_filter

    logger.info(f"Applying BGM mixing: {bgm_path.name}")

    video_duration = _get_media_duration(video_path)
    bgm_duration = _get_media_duration(bgm_path)
    has_audio = _has_audio_stream(video_path)

    logger.info(
        f"Video duration: {video_duration:.2f}s, BGM duration: {bgm_duration:.2f}s, "
        f"has_audio: {has_audio}"
    )

    bgm_filter = create_bgm_filter(
        bgm_duration=bgm_duration,
        video_duration=video_duration,
        bgm_volume=bgm_volume,
        bgm_loop=bgm_loop,
        has_audio=has_audio,
    )

    cmd = [
        "ffmpeg",
        "-y",
        "-i",
        str(video_path),
        "-i",
        str(bgm_path),
        "-filter_complex",
        bgm_filter,
        "-map",
        "0:v",
        "-map",
        "[a_out]",
        "-c:v",
        "copy",
        "-c:a",
        "aac",
        "-b:a",
        "320k",
        str(output_path),
    ]

    logger.info(f"Running BGM mixing: {' '.join(cmd)}")

    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        logger.error(f"BGM mixing failed: {result.stderr}")
        raise RuntimeError(f"BGM mixing failed: {result.stderr}")

    logger.info(f"BGM mixing completed: {output_path}")
    return output_path


def handle_single_file_upload(
    video_file: VideoFile,
    args: ValidatedArgs,
) -> Path:
    """
    ë‹¨ì¼ íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œ ì²˜ë¦¬.

    ì¸ì½”ë”©/ë³‘í•© ì—†ì´ DB ì €ì¥ í›„ ì›ë³¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜.

    Args:
        video_file: VideoFile ê°ì²´
        args: ê²€ì¦ëœ CLI ì¸ì

    Returns:
        ì›ë³¸ íŒŒì¼ ê²½ë¡œ
    """
    logger.info(f"Single file detected with --upload, skipping transcode: {video_file.path.name}")

    # 1. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    metadata = detect_metadata(video_file.path)

    # 2. YouTube ì œëª© ìƒì„± (ë””ë ‰í† ë¦¬ëª… ê¸°ë°˜)
    title = get_output_filename([video_file.path]).replace(".mp4", "")

    # 3. ì´¬ì˜ ì‹œê°„ ì¶”ì¶œ
    creation_time_str = video_file.creation_time.strftime("%H:%M:%S")

    # 4. í´ë¦½ ì •ë³´ ìƒì„±
    clip = ClipInfo(
        name=video_file.path.name,
        duration=metadata.duration_seconds,
        device=metadata.device_model or "Unknown",
        shot_time=creation_time_str,
    )

    # 5. YouTube ì„¤ëª… ìƒì„± (ë‹¨ì¼ íŒŒì¼ìš©)
    youtube_description = generate_single_file_description(
        device=clip.device, shot_time=clip.shot_time
    )

    # 6. DB ì €ì¥ (íƒ€ì„ë¼ì¸ dict: start/end í¬í•¨)
    clip_dict: dict[str, str | float | None] = {
        "name": clip.name,
        "duration": clip.duration,
        "start": 0.0,
        "end": clip.duration,
        "device": clip.device,
        "shot_time": clip.shot_time,
    }
    with database_session() as conn:
        repo = MergeJobRepository(conn)
        today = date.today().isoformat()

        repo.create(
            output_path=video_file.path,
            video_ids=[],  # íŠ¸ëœìŠ¤ì½”ë”© ì•ˆ í•¨
            title=title,
            date=today,
            total_duration_seconds=metadata.duration_seconds,
            total_size_bytes=video_file.path.stat().st_size,
            clips_info_json=json.dumps([clip_dict]),
            summary_markdown=youtube_description,
        )

    # 7. ì½˜ì†” ì¶œë ¥
    logger.info(f"Saved to DB: {title}")
    print("\nğŸ“ ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ (íŠ¸ëœìŠ¤ì½”ë”© ìƒëµ)")
    print(f"ğŸ“¹ íŒŒì¼: {video_file.path.name}")
    minutes = int(metadata.duration_seconds // 60)
    seconds = int(metadata.duration_seconds % 60)
    print(f"â±ï¸  ê¸¸ì´: {minutes}ë¶„ {seconds}ì´ˆ")
    if metadata.device_model:
        print(f"ğŸ“· ê¸°ê¸°: {metadata.device_model}")

    return video_file.path


def _collect_clip_info(video_file: VideoFile) -> ClipInfo:
    """ì˜ìƒ íŒŒì¼ì—ì„œ SummaryÂ·íƒ€ì„ë¼ì¸ìš© í´ë¦½ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.

    ffprobeë¡œ í•´ìƒë„Â·ì½”ë±Â·ê¸¸ì´ ë“±ì„ ì¶”ì¶œí•˜ê³ , íŒŒì¼ ìƒì„± ì‹œê°„ì—ì„œ
    ì´¬ì˜ ì‹œê° ë¬¸ìì—´ì„ ë§Œë“ ë‹¤. ffprobe ì‹¤íŒ¨ ì‹œ duration=0.0 í´ë°±.

    Args:
        video_file: ëŒ€ìƒ ì˜ìƒ íŒŒì¼

    Returns:
        ClipInfo(name, duration, device, shot_time)
    """
    try:
        metadata = detect_metadata(video_file.path)
        creation_time_str = video_file.creation_time.strftime("%H:%M:%S")
        return ClipInfo(
            name=video_file.path.name,
            duration=metadata.duration_seconds,
            device=metadata.device_model,
            shot_time=creation_time_str,
        )
    except Exception as e:
        logger.warning(f"Failed to get metadata for {video_file.path}: {e}")
        return ClipInfo(name=video_file.path.name, duration=0.0, device=None, shot_time=None)


def _transcode_single(
    video_file: VideoFile,
    temp_dir: Path,
    opts: TranscodeOptions,
) -> TranscodeResult:
    """ë‹¨ì¼ íŒŒì¼ì„ ë…ë¦½ Transcoder ì»¨í…ìŠ¤íŠ¸ì—ì„œ íŠ¸ëœìŠ¤ì½”ë”©í•œë‹¤.

    ``_transcode_parallel`` ì—ì„œ ThreadPoolExecutorì— ì œì¶œë˜ëŠ” ë‹¨ìœ„ ì‘ì—…ì´ë‹¤.
    ê° í˜¸ì¶œë§ˆë‹¤ Transcoderë¥¼ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ë³´ì¥í•œë‹¤.

    Args:
        video_file: íŠ¸ëœìŠ¤ì½”ë”©í•  ì›ë³¸ ì˜ìƒ
        temp_dir: íŠ¸ëœìŠ¤ì½”ë”© ì¶œë ¥ ì„ì‹œ ë””ë ‰í† ë¦¬
        opts: ê³µí†µ íŠ¸ëœìŠ¤ì½”ë”© ì˜µì…˜ (denoise, loudnorm, fade ë“±)

    Returns:
        ``TranscodeResult`` (ì¶œë ¥ ê²½ë¡œ, video DB ID, í´ë¦½ ë©”íƒ€ë°ì´í„°)
    """
    fade_config = opts.fade_map.get(video_file.path) if opts.fade_map else None
    fade_in = fade_config.fade_in if fade_config else None
    fade_out = fade_config.fade_out if fade_config else None

    with Transcoder(temp_dir=temp_dir) as transcoder:
        output_path, video_id, silence_segments = transcoder.transcode_video(
            video_file,
            denoise=opts.denoise,
            denoise_level=opts.denoise_level,
            normalize_audio=opts.normalize_audio,
            fade_duration=opts.fade_duration,
            fade_in_duration=fade_in,
            fade_out_duration=fade_out,
            trim_silence=opts.trim_silence,
            silence_threshold=opts.silence_threshold,
            silence_min_duration=opts.silence_min_duration,
            stabilize=opts.stabilize,
            stabilize_strength=opts.stabilize_strength,
            stabilize_crop=opts.stabilize_crop,
            lut_path=str(opts.lut_path) if opts.lut_path else None,
            auto_lut=opts.auto_lut,
            lut_before_hdr=opts.lut_before_hdr,
            device_luts=opts.device_luts,
        )
        clip_info = _collect_clip_info(video_file)
        return TranscodeResult(
            output_path=output_path,
            video_id=video_id,
            clip_info=clip_info,
            silence_segments=silence_segments,
        )


def _transcode_parallel(
    video_files: list[VideoFile],
    temp_dir: Path,
    max_workers: int,
    opts: TranscodeOptions,
) -> list[TranscodeResult]:
    """``ThreadPoolExecutor`` ë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ íŠ¸ëœìŠ¤ì½”ë”©.

    ê° íŒŒì¼ì„ ë…ë¦½ëœ :class:`Transcoder` ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²˜ë¦¬í•˜ë©°,
    ì™„ë£Œ ìˆœì„œì— ê´€ê³„ì—†ì´ **ì›ë³¸ ì¸ë±ìŠ¤ ìˆœ** ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•œë‹¤.

    Args:
        video_files: íŠ¸ëœìŠ¤ì½”ë”© ëŒ€ìƒ íŒŒì¼ ëª©ë¡
        temp_dir: ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬
        max_workers: ìµœëŒ€ ë™ì‹œ ì›Œì»¤ ìˆ˜
        opts: íŠ¸ëœìŠ¤ì½”ë”© ê³µí†µ ì˜µì…˜ (denoise, loudnorm, fade ë“±)

    Returns:
        ì›ë³¸ ìˆœì„œê°€ ìœ ì§€ëœ íŠ¸ëœìŠ¤ì½”ë”© ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Raises:
        RuntimeError: í•˜ë‚˜ ì´ìƒì˜ ì›Œì»¤ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
    """
    results: dict[int, TranscodeResult] = {}
    completed_count = 0
    total_count = len(video_files)
    print_lock = Lock()

    def on_complete(idx: int, filename: str, status: str) -> None:
        """ë³‘ë ¬ ì›Œì»¤ ì™„ë£Œ ì½œë°± -- ì§„í–‰ ì¹´ìš´í„° ê°±ì‹  ë° ì½˜ì†” ì¶œë ¥."""
        nonlocal completed_count
        with print_lock:
            completed_count += 1
            print(
                f"\rğŸ¬ íŠ¸ëœìŠ¤ì½”ë”©: [{completed_count}/{total_count}] {status}: {filename}",
                end="",
                flush=True,
            )
            if completed_count == total_count:
                print()  # ì¤„ë°”ê¿ˆ

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(
                _transcode_single,
                video_file,
                temp_dir,
                opts,
            ): i
            for i, video_file in enumerate(video_files)
        }
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                results[idx] = result
                on_complete(idx, video_files[idx].path.name, "ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Failed to transcode {video_files[idx].path}: {e}")
                on_complete(idx, video_files[idx].path.name, "ì‹¤íŒ¨")
                raise

    return [results[i] for i in range(total_count)]


def _transcode_sequential(
    video_files: list[VideoFile],
    temp_dir: Path,
    opts: TranscodeOptions,
) -> list[TranscodeResult]:
    """ì˜ìƒ íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ëœìŠ¤ì½”ë”©í•œë‹¤.

    :class:`MultiProgressBar` ë¡œ íŒŒì¼ë³„ ì§„í–‰ë¥ (fps, ETA)ì„ ì‹¤ì‹œê°„ í‘œì‹œí•œë‹¤.
    ``parallel=1`` ì´ê±°ë‚˜ íŒŒì¼ì´ 1ê°œì¼ ë•Œ ì‚¬ìš©ëœë‹¤.

    Args:
        video_files: íŠ¸ëœìŠ¤ì½”ë”©í•  ì˜ìƒ ëª©ë¡
        temp_dir: íŠ¸ëœìŠ¤ì½”ë”© ê²°ê³¼ ì €ì¥ ì„ì‹œ ë””ë ‰í† ë¦¬
        opts: íŠ¸ëœìŠ¤ì½”ë”© ê³µí†µ ì˜µì…˜ (ì˜¤ë””ì˜¤Â·í˜ì´ë“œ ì„¤ì •)

    Returns:
        íŠ¸ëœìŠ¤ì½”ë”© ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì¶œë ¥ ê²½ë¡œ, video_id, í´ë¦½ ì •ë³´)
    """
    results: list[TranscodeResult] = []
    progress = MultiProgressBar(total_files=len(video_files))

    with Transcoder(temp_dir=temp_dir) as transcoder:
        for video_file in video_files:
            progress.start_file(video_file.path.name)

            def on_progress_info(info: ProgressInfo) -> None:
                """FFmpeg ìƒì„¸ ì§„í–‰ë¥ ì„ MultiProgressBarì— ì „ë‹¬."""
                progress.update_with_info(info)

            fade_config = opts.fade_map.get(video_file.path) if opts.fade_map else None
            fade_in = fade_config.fade_in if fade_config else None
            fade_out = fade_config.fade_out if fade_config else None

            output_path, video_id, silence_segments = transcoder.transcode_video(
                video_file,
                denoise=opts.denoise,
                denoise_level=opts.denoise_level,
                normalize_audio=opts.normalize_audio,
                fade_duration=opts.fade_duration,
                fade_in_duration=fade_in,
                fade_out_duration=fade_out,
                trim_silence=opts.trim_silence,
                silence_threshold=opts.silence_threshold,
                silence_min_duration=opts.silence_min_duration,
                stabilize=opts.stabilize,
                stabilize_strength=opts.stabilize_strength,
                stabilize_crop=opts.stabilize_crop,
                lut_path=str(opts.lut_path) if opts.lut_path else None,
                auto_lut=opts.auto_lut,
                lut_before_hdr=opts.lut_before_hdr,
                device_luts=opts.device_luts,
                progress_info_callback=on_progress_info,
            )
            clip_info = _collect_clip_info(video_file)
            results.append(
                TranscodeResult(
                    output_path=output_path,
                    video_id=video_id,
                    clip_info=clip_info,
                    silence_segments=silence_segments,
                )
            )
            progress.finish_file()

    return results


def _apply_ordering(
    video_files: list[VideoFile],
    validated_args: ValidatedArgs,
    *,
    allow_interactive: bool = True,
) -> list[VideoFile]:
    """í•„í„°ë§Â·ì •ë ¬Â·ì¸í„°ë™í‹°ë¸Œ ì¬ì •ë ¬ì„ ìˆœì°¨ ì ìš©í•œë‹¤.

    Args:
        video_files: ìŠ¤ìº”ëœ ì˜ìƒ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        validated_args: ê²€ì¦ëœ CLI ì¸ì
        allow_interactive: ``--reorder`` ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ í—ˆìš© ì—¬ë¶€
            (dry-runì—ì„œëŠ” False)

    Returns:
        ìµœì¢… ìˆœì„œì˜ ì˜ìƒ íŒŒì¼ ë¦¬ìŠ¤íŠ¸

    Raises:
        ValueError: í•„í„° ì ìš© í›„ íŒŒì¼ì´ ì—†ê±°ë‚˜ ì¬ì •ë ¬ í›„ íŒŒì¼ì´ ì—†ì„ ë•Œ
    """
    if validated_args.exclude_patterns or validated_args.include_only_patterns:
        video_files = filter_videos(
            video_files,
            exclude_patterns=validated_args.exclude_patterns,
            include_only_patterns=validated_args.include_only_patterns,
        )
        if not video_files:
            raise ValueError("All files excluded by filter patterns")

    if validated_args.sort_key != "time":
        video_files = sort_videos(video_files, SortKey(validated_args.sort_key))

    if allow_interactive and validated_args.reorder:
        video_files = interactive_reorder(video_files)
        if not video_files:
            raise ValueError("No files remaining after reorder")

    return video_files


def _resolve_output_path(validated_args: ValidatedArgs) -> Path:
    """ì¶œë ¥ íŒŒì¼ ê²½ë¡œë¥¼ ê²°ì •í•œë‹¤.

    ìš°ì„ ìˆœìœ„: ``--output`` ì§ì ‘ ì§€ì • > ``--output-dir`` + ìë™ íŒŒì¼ëª….

    Args:
        validated_args: ê²€ì¦ëœ CLI ì¸ì

    Returns:
        ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    if validated_args.output:
        return validated_args.output
    output_filename = get_output_filename(validated_args.targets)
    output_dir = validated_args.output_dir or Path.cwd()
    return output_dir / output_filename


def _cleanup_temp(
    temp_dir: Path,
    results: list[TranscodeResult],
    final_path: Path,
    video_ids: list[int],
) -> None:
    """ì„ì‹œ íŒŒì¼ ë° í´ë”ë¥¼ ì •ë¦¬í•˜ê³  DB ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤."""
    logger.info("Cleaning up temporary files...")
    for r in results:
        if r.output_path.exists() and r.output_path != final_path:
            r.output_path.unlink()
            logger.debug(f"  Removed: {r.output_path}")

    # DB ìƒíƒœ ì—…ë°ì´íŠ¸: completed â†’ merged
    _mark_transcoding_jobs_merged(video_ids)

    # ì„ì‹œ í´ë” ì‚­ì œ
    if temp_dir.exists():
        try:
            shutil.rmtree(temp_dir)
            logger.info(f"Removed temp directory: {temp_dir}")
        except OSError as e:
            logger.warning(f"Failed to remove temp directory: {e}")


def _print_summary(summary_markdown: str | None) -> None:
    """ë³‘í•© ìš”ì•½ ë§ˆí¬ë‹¤ìš´ì„ êµ¬ë¶„ì„ ê³¼ í•¨ê»˜ ì½˜ì†”ì— ì¶œë ¥í•œë‹¤.

    Args:
        summary_markdown: ì¶œë ¥í•  ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´. ``None`` ì´ë©´ ë¬´ì‹œ.
    """
    if not summary_markdown:
        return
    print("\n" + "=" * 60)
    print("ğŸ“‹ SUMMARY (Copy & Paste)")
    print("=" * 60)
    print(summary_markdown)
    print("=" * 60 + "\n")


def run_pipeline(
    validated_args: ValidatedArgs,
    notifier: Notifier | None = None,
    generated_thumbnail_paths: list[Path] | None = None,
) -> Path:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.

    ìŠ¤ìº” â†’ íŠ¸ëœìŠ¤ì½”ë”© â†’ ë³‘í•© â†’ DB ì €ì¥ â†’ ì •ë¦¬ â†’ Summary ì¶œë ¥

    Args:
        validated_args: ê²€ì¦ëœ ì¸ì
        notifier: ì•Œë¦¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (Noneì´ë©´ ì•Œë¦¼ ë¹„í™œì„±í™”)
        generated_thumbnail_paths: ì¸ë„¤ì¼ ìƒì„± ê²°ê³¼ ì €ì¥ìš© ì¶œë ¥ ë²„í¼ (ê¸°ë³¸ê°’ None)

    Returns:
        ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # 1. íŒŒì¼ ìŠ¤ìº”
    logger.info("Scanning video files...")
    video_files = scan_videos(validated_args.targets)

    if not video_files:
        logger.error("No video files found")
        raise ValueError("No video files found")

    logger.info(f"Found {len(video_files)} video files")
    for video_file in video_files:
        logger.info(f"  - {video_file.path.name}")

    video_files = _apply_ordering(video_files, validated_args)

    # --detect-silence: ë¶„ì„ë§Œ ìˆ˜í–‰ í›„ ì¢…ë£Œ
    if validated_args.detect_silence:
        _detect_silence_only(video_files, validated_args)
        return Path()  # ë¹ˆ ê²½ë¡œ ë°˜í™˜

    # ë‹¨ì¼ íŒŒì¼ + --upload ì‹œ ë¹ ë¥¸ ê²½ë¡œ
    if len(video_files) == 1 and validated_args.upload:
        return handle_single_file_upload(video_files[0], validated_args)

    # 1.5 ê·¸ë£¹í•‘ ë° ì¬ì •ë ¬
    if validated_args.group_sequences:
        groups = group_sequences(video_files)
        video_files = reorder_with_groups(video_files, groups)
        for group in groups:
            if len(group.files) > 1:
                logger.info(
                    "ì—°ì† ì‹œí€€ìŠ¤ ê°ì§€: %s (%dê°œ íŒŒì¼)",
                    group.group_id,
                    len(group.files),
                )
    else:
        groups = [
            FileSequenceGroup(files=(video_file,), group_id=f"s_{i}")
            for i, video_file in enumerate(video_files)
        ]

    fade_map = compute_fade_map(groups, default_fade=validated_args.fade_duration)

    # 2. íŠ¸ëœìŠ¤ì½”ë”©
    temp_dir = get_temp_dir()
    logger.info(f"Using temp directory: {temp_dir}")

    transcode_opts = TranscodeOptions(
        denoise=validated_args.denoise,
        denoise_level=validated_args.denoise_level,
        normalize_audio=validated_args.normalize_audio,
        fade_map=fade_map,
        fade_duration=validated_args.fade_duration,
        trim_silence=validated_args.trim_silence,
        silence_threshold=validated_args.silence_threshold,
        silence_min_duration=validated_args.silence_min_duration,
        stabilize=validated_args.stabilize,
        stabilize_strength=validated_args.stabilize_strength,
        stabilize_crop=validated_args.stabilize_crop,
        lut_path=validated_args.lut_path,
        auto_lut=validated_args.auto_lut,
        lut_before_hdr=validated_args.lut_before_hdr,
        device_luts=validated_args.device_luts,
    )

    if validated_args.stabilize:
        logger.info(
            "ì˜ìƒ ì•ˆì •í™” í™œì„±í™” (vidstab 2-pass, strength=%s, crop=%s) "
            "â€” íŠ¸ëœìŠ¤ì½”ë”© ì‹œê°„ì´ ì¦ê°€í•©ë‹ˆë‹¤",
            validated_args.stabilize_strength,
            validated_args.stabilize_crop,
        )

    parallel = validated_args.parallel
    if parallel > 1:
        logger.info(f"Starting parallel transcoding (workers: {parallel})...")
        results = _transcode_parallel(video_files, temp_dir, parallel, transcode_opts)
    else:
        logger.info("Starting transcoding...")
        results = _transcode_sequential(video_files, temp_dir, transcode_opts)

    # ì•Œë¦¼: íŠ¸ëœìŠ¤ì½”ë”© ì™„ë£Œ
    if notifier:
        from tubearchive.notification import transcode_complete_event

        notifier.notify(
            transcode_complete_event(
                file_count=len(results),
                total_duration=sum(r.clip_info.duration for r in results),
            )
        )

    # 3. ë³‘í•©
    logger.info("Merging videos...")
    output_path = _resolve_output_path(validated_args)
    final_path = Merger(temp_dir=temp_dir).merge(
        [r.output_path for r in results],
        output_path,
    )
    logger.info(f"Final output: {final_path}")

    # ì•Œë¦¼: ë³‘í•© ì™„ë£Œ
    if notifier:
        from tubearchive.notification import merge_complete_event

        notifier.notify(
            merge_complete_event(
                output_path=str(final_path),
                file_count=len(results),
                total_size_bytes=final_path.stat().st_size if final_path.exists() else 0,
            )
        )

    # 3.5 BGM ë¯¹ì‹± (ì˜µì…˜)
    if validated_args.bgm_path:
        logger.info("Applying BGM mixing...")
        temp_bgm_output = temp_dir / f"bgm_mixed_{final_path.name}"
        bgm_mixed_path = _apply_bgm_mixing(
            video_path=final_path,
            bgm_path=validated_args.bgm_path,
            bgm_volume=validated_args.bgm_volume,
            bgm_loop=validated_args.bgm_loop,
            output_path=temp_bgm_output,
        )
        # ì›ë³¸ì„ BGM ë¯¹ì‹±ëœ íŒŒì¼ë¡œ ëŒ€ì²´
        shutil.move(str(bgm_mixed_path), str(final_path))
        logger.info(f"BGM mixing applied: {final_path}")

    # 4.1 í™”ì§ˆ ë¦¬í¬íŠ¸ ì¶œë ¥ (ì„ íƒ)
    if validated_args.quality_report:
        _print_quality_report(video_files, results)

    # 4. DB ì €ì¥ ë° Summary ìƒì„±
    video_ids = [r.video_id for r in results]
    video_clips = [r.clip_info for r in results]
    summary, merge_job_id = save_merge_job_to_db(
        final_path,
        video_clips,
        validated_args.targets,
        video_ids,
        groups=groups,
    )

    # 4.1 í”„ë¡œì íŠ¸ ì—°ê²° (--project ì˜µì…˜ ì‹œ)
    if validated_args.project and merge_job_id is not None:
        _link_merge_job_to_project(validated_args.project, merge_job_id)

    # 4.5 ì¸ë„¤ì¼ ìƒì„± (ë¹„í•„ìˆ˜)
    if generated_thumbnail_paths is not None:
        generated_thumbnail_paths.clear()

    if validated_args.thumbnail:
        thumbnail_paths = _generate_thumbnails(final_path, validated_args)
        if generated_thumbnail_paths is not None:
            generated_thumbnail_paths.extend(thumbnail_paths)
        if thumbnail_paths:
            print(f"\nğŸ–¼ï¸  ì¸ë„¤ì¼ {len(thumbnail_paths)}ì¥ ìƒì„±:")
            for tp in thumbnail_paths:
                print(f"  - {tp}")

    # 4.6 ì˜ìƒ ë¶„í•  (ë¹„í•„ìˆ˜)
    if validated_args.split_duration or validated_args.split_size:
        from tubearchive.core.splitter import SplitOptions, VideoSplitter

        splitter = VideoSplitter()
        split_opts = SplitOptions(
            duration=(
                splitter.parse_duration(validated_args.split_duration)
                if validated_args.split_duration
                else None
            ),
            size=(
                splitter.parse_size(validated_args.split_size)
                if validated_args.split_size
                else None
            ),
        )

        split_output_dir = final_path.parent
        split_criterion = "duration" if split_opts.duration else "size"
        split_value = validated_args.split_duration or validated_args.split_size or ""
        logger.info("Splitting video...")
        try:
            split_files = splitter.split_video(final_path, split_output_dir, split_opts)
            if split_files:
                print(f"\nâœ‚ï¸  ì˜ìƒ {len(split_files)}ê°œë¡œ ë¶„í• :")
                for sf in split_files:
                    file_size = sf.stat().st_size if sf.exists() else 0
                    size_str = format_size(file_size)
                    print(f"  - {sf.name} ({size_str})")

                # DBì— split job ì €ì¥
                if merge_job_id is not None:
                    try:
                        with database_session() as conn:
                            split_repo = SplitJobRepository(conn)
                            split_repo.create(
                                merge_job_id=merge_job_id,
                                split_criterion=split_criterion,
                                split_value=split_value,
                                output_files=split_files,
                            )
                        logger.debug("Split job saved to database")
                    except Exception as e:
                        logger.warning(f"Failed to save split job to DB: {e}")
        except Exception as e:
            logger.warning(f"Failed to split video: {e}")
            print(f"\nâš ï¸  ì˜ìƒ ë¶„í•  ì‹¤íŒ¨: {e}")

    # 4.7 íƒ€ì„ë©ìŠ¤ ìƒì„± (ë¹„í•„ìˆ˜)
    timelapse_path: Path | None = None
    if validated_args.timelapse_speed:
        timelapse_path = _generate_timelapse(final_path, validated_args)
        if timelapse_path:
            print(f"\nâ© íƒ€ì„ë©ìŠ¤ ({validated_args.timelapse_speed}x) ìƒì„±:")
            print(f"  - {timelapse_path}")
    # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if not validated_args.keep_temp:
        _cleanup_temp(temp_dir, results, final_path, video_ids)

    # 5.5 ì›ë³¸ íŒŒì¼ ì•„ì¹´ì´ë¹™ (CLI ì˜µì…˜ ë˜ëŠ” config ì •ì±…)
    video_paths_for_archive = [
        (r.video_id, vf.path) for r, vf in zip(results, video_files, strict=True)
    ]
    _archive_originals(video_paths_for_archive, validated_args)

    # 6. Summary ì¶œë ¥
    _print_summary(summary)

    return final_path


def _archive_originals(
    video_paths: list[tuple[int, Path]],
    validated_args: ValidatedArgs,
) -> None:
    """ì›ë³¸ íŒŒì¼ë“¤ì„ ì •ì±…ì— ë”°ë¼ ì•„ì¹´ì´ë¹™í•œë‹¤.

    CLI ì˜µì…˜(``--archive-originals``) ë˜ëŠ” ì„¤ì • íŒŒì¼(``[archive]``)ì˜
    ì •ì±…ì„ ì½ì–´ ì›ë³¸ íŒŒì¼ì„ ì´ë™/ì‚­ì œ/ìœ ì§€í•œë‹¤.

    ìš°ì„ ìˆœìœ„: CLI ``--archive-originals`` > config ``[archive].policy``

    Args:
        video_paths: (video_id, original_path) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        validated_args: ê²€ì¦ëœ CLI ì¸ì
    """
    from tubearchive.config import get_default_archive_destination, get_default_archive_policy
    from tubearchive.core.archiver import ArchivePolicy, Archiver

    if not video_paths:
        logger.warning("ì•„ì¹´ì´ë¹™í•  ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì •ì±… ê²°ì •: CLI ì˜µì…˜ > config > ê¸°ë³¸ê°’(KEEP)
    if validated_args.archive_originals:
        policy = ArchivePolicy.MOVE
        destination: Path | None = validated_args.archive_originals
    else:
        policy_str = get_default_archive_policy()
        policy = ArchivePolicy(policy_str)
        destination = get_default_archive_destination()

    # KEEP ì •ì±…ì´ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    if policy == ArchivePolicy.KEEP:
        logger.debug("ì•„ì¹´ì´ë¸Œ ì •ì±…ì´ KEEPì…ë‹ˆë‹¤. ì›ë³¸ íŒŒì¼ ìœ ì§€.")
        return

    # MOVE ì •ì±…ì¸ë° destinationì´ ì—†ìœ¼ë©´ ê²½ê³ 
    if policy == ArchivePolicy.MOVE and not destination:
        logger.warning("MOVE ì •ì±…ì´ ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ destinationì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ íŒŒì¼ ìœ ì§€.")
        return

    # DELETE ì •ì±… ì‹œ í™•ì¸ í”„ë¡¬í”„íŠ¸ (core ëª¨ë“ˆì´ ì•„ë‹Œ CLI ê³„ì¸µì—ì„œ ì²˜ë¦¬)
    if (
        policy == ArchivePolicy.DELETE
        and not validated_args.archive_force
        and not _prompt_archive_delete_confirmation(len(video_paths))
    ):
        logger.info("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return

    logger.info("ì›ë³¸ íŒŒì¼ ì•„ì¹´ì´ë¹™ ì‹œì‘ (ì •ì±…: %s)...", policy.value)

    with database_session() as conn:
        from tubearchive.database.repository import ArchiveHistoryRepository

        archive_repo = ArchiveHistoryRepository(conn)
        archiver = Archiver(
            repo=archive_repo,
            policy=policy,
            destination=destination,
        )
        stats = archiver.archive_files(video_paths)

    if policy == ArchivePolicy.MOVE:
        logger.info("ì•„ì¹´ì´ë¹™ ì™„ë£Œ: ì´ë™ %d, ì‹¤íŒ¨ %d", stats.moved, stats.failed)
    elif policy == ArchivePolicy.DELETE:
        logger.info("ì•„ì¹´ì´ë¹™ ì™„ë£Œ: ì‚­ì œ %d, ì‹¤íŒ¨ %d", stats.deleted, stats.failed)


def _prompt_archive_delete_confirmation(file_count: int) -> bool:
    """ì›ë³¸ íŒŒì¼ ì‚­ì œ í™•ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ í‘œì‹œí•œë‹¤.

    Args:
        file_count: ì‚­ì œ ëŒ€ìƒ íŒŒì¼ ê°œìˆ˜

    Returns:
        True: ì‚­ì œ ìŠ¹ì¸, False: ì·¨ì†Œ
    """
    print(f"\nâš ï¸  {file_count}ê°œì˜ ì›ë³¸ íŒŒì¼ì„ ì˜êµ¬ ì‚­ì œí•˜ë ¤ê³  í•©ë‹ˆë‹¤.")
    print("ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    return response in {"y", "yes"}


def _detect_silence_only(
    video_files: list[VideoFile],
    validated_args: ValidatedArgs,
) -> None:
    """
    ë¬´ìŒ êµ¬ê°„ ê°ì§€ ì „ìš© ëª¨ë“œ.

    ê° ì˜ìƒì˜ ë¬´ìŒ êµ¬ê°„ì„ ê°ì§€í•˜ê³  ì½˜ì†”ì— ì¶œë ¥í•œë‹¤.
    """
    from tubearchive.ffmpeg.effects import (
        create_silence_detect_filter,
        parse_silence_segments,
    )
    from tubearchive.ffmpeg.executor import FFmpegExecutor

    executor = FFmpegExecutor()

    threshold = validated_args.silence_threshold
    min_duration = validated_args.silence_min_duration

    for video_file in video_files:
        print(f"\nğŸ” ë¶„ì„ ì¤‘: {video_file.path.name}")

        # silencedetect í•„í„° ìƒì„±
        detect_filter = create_silence_detect_filter(
            threshold=threshold,
            min_duration=min_duration,
        )

        # ë¶„ì„ ëª…ë ¹ ì‹¤í–‰
        cmd = executor.build_silence_detection_command(
            input_path=video_file.path,
            audio_filter=detect_filter,
        )
        stderr = executor.run_analysis(cmd)

        # íŒŒì‹±
        segments = parse_silence_segments(stderr)

        if not segments:
            print("  ë¬´ìŒ êµ¬ê°„ ì—†ìŒ")
        else:
            print(f"  ë¬´ìŒ êµ¬ê°„ {len(segments)}ê°œ ë°œê²¬:")
            for i, seg in enumerate(segments, 1):
                print(f"    {i}. {seg.start:.2f}s - {seg.end:.2f}s (ê¸¸ì´: {seg.duration:.2f}s)")


def _generate_thumbnails(
    video_path: Path,
    validated_args: ValidatedArgs,
) -> list[Path]:
    """ë³‘í•© ì˜ìƒì—ì„œ ì¸ë„¤ì¼ ìƒì„±.

    ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ ì—†ìŒ).
    """
    from tubearchive.ffmpeg.thumbnail import extract_thumbnails, parse_timestamp

    timestamps: list[float] | None = None
    if validated_args.thumbnail_timestamps:
        parsed: list[float] = []
        for ts in validated_args.thumbnail_timestamps:
            try:
                parsed.append(parse_timestamp(ts))
            except ValueError as e:
                logger.warning("Invalid thumbnail timestamp '%s': %s", ts, e)
        timestamps = parsed if parsed else None

    try:
        return extract_thumbnails(
            video_path,
            timestamps=timestamps,
            output_dir=validated_args.output_dir,
            quality=validated_args.thumbnail_quality,
        )
    except Exception:
        logger.warning("Failed to generate thumbnails", exc_info=True)
        return []


def _print_quality_report(
    video_files: list[VideoFile],
    results: list[TranscodeResult],
) -> None:
    """íŠ¸ëœìŠ¤ì½”ë”© ì „/í›„ SSIM/PSNR/VMAF ì§€í‘œë¥¼ ì¶œë ¥í•œë‹¤."""
    from tubearchive.core.quality import generate_quality_reports

    pairs = [
        (source.path, result.output_path)
        for source, result in zip(video_files, results, strict=True)
    ]
    reports = generate_quality_reports(pairs)
    if not reports:
        print("\nğŸ”¬ í™”ì§ˆ ë¦¬í¬íŠ¸: ê³„ì‚° ëŒ€ìƒ ì—†ìŒ")
        return

    print("\nğŸ”¬ í™”ì§ˆ ë¦¬í¬íŠ¸:")
    for report in reports:
        print(f"\n  - ì›ë³¸: {report.source_path.name}")
        print(f"    ê²°ê³¼: {report.output_path.name}")
        if report.ssim is not None:
            print(f"    SSIM: {report.ssim:.4f}")
        if report.psnr is not None:
            print(f"    PSNR: {report.psnr:.4f} dB")
        if report.vmaf is not None:
            print(f"    VMAF: {report.vmaf:.4f}")

        if report.unavailable:
            missing = ", ".join(sorted(report.unavailable))
            print(f"    ë¯¸ì§€ì›/ì‹¤íŒ¨ ì§€í‘œ: {missing}")
        if report.errors:
            for err in report.errors:
                print(f"    ê²½ê³ : {err}")


def _generate_timelapse(
    video_path: Path,
    validated_args: ValidatedArgs,
) -> Path | None:
    """ë³‘í•© ì˜ìƒì—ì„œ íƒ€ì„ë©ìŠ¤ ìƒì„±.

    ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ë‚¨ê¸°ê³  None ë°˜í™˜ (íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ ì—†ìŒ).

    Args:
        video_path: ì…ë ¥ ë³‘í•© ì˜ìƒ ê²½ë¡œ
        validated_args: ê²€ì¦ëœ CLI ì¸ì

    Returns:
        íƒ€ì„ë©ìŠ¤ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
    """
    from tubearchive.core.timelapse import TimelapseGenerator

    if validated_args.timelapse_speed is None:
        return None

    # ì¶œë ¥ ê²½ë¡œ ìƒì„±
    stem = video_path.stem
    suffix = video_path.suffix
    output_dir = validated_args.output_dir or video_path.parent
    output_path = output_dir / f"{stem}_timelapse_{validated_args.timelapse_speed}x{suffix}"

    try:
        logger.info(f"Generating {validated_args.timelapse_speed}x timelapse: {output_path.name}")
        generator = TimelapseGenerator()
        return generator.generate(
            input_path=video_path,
            output_path=output_path,
            speed=validated_args.timelapse_speed,
            keep_audio=validated_args.timelapse_audio,
            resolution=validated_args.timelapse_resolution,
        )
    except Exception:
        logger.warning("Failed to generate timelapse", exc_info=True)
        return None


def _mark_transcoding_jobs_merged(video_ids: list[int]) -> None:
    """íŠ¸ëœìŠ¤ì½”ë”© ì‘ì—… ìƒíƒœë¥¼ mergedë¡œ ì—…ë°ì´íŠ¸ (ì„ì‹œ íŒŒì¼ ì •ë¦¬ í›„)."""
    if not video_ids:
        return
    try:
        with database_session() as conn:
            job_repo = TranscodingJobRepository(conn)
            count = job_repo.mark_merged_by_video_ids(video_ids)
        logger.debug(f"Marked {count} transcoding jobs as merged")
    except Exception:
        logger.warning("Failed to mark transcoding jobs as merged", exc_info=True)


def save_merge_job_to_db(
    output_path: Path,
    video_clips: list[ClipInfo],
    targets: list[Path],
    video_ids: list[int],
    groups: list[FileSequenceGroup] | None = None,
) -> tuple[str | None, int | None]:
    """ë³‘í•© ì‘ì—… ì •ë³´ë¥¼ DBì— ì €ì¥ (íƒ€ì„ë¼ì¸ ë° Summary í¬í•¨).

    Args:
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        video_clips: í´ë¦½ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        targets: ì…ë ¥ íƒ€ê²Ÿ ëª©ë¡ (ì œëª© ì¶”ì¶œìš©)
        video_ids: ë³‘í•©ëœ ì˜ìƒë“¤ì˜ DB ID ëª©ë¡
        groups: ì‹œí€€ìŠ¤ ê·¸ë£¹ ëª©ë¡ (Summary ìƒì„±ìš©)

    Returns:
        (ì½˜ì†” ì¶œë ¥ìš© Summary ë§ˆí¬ë‹¤ìš´, merge_job_id) íŠœí”Œ. ì‹¤íŒ¨ ì‹œ (None, None).
    """
    from tubearchive.utils.summary_generator import (
        generate_clip_summary,
        generate_youtube_description,
    )

    try:
        with database_session() as conn:
            repo = MergeJobRepository(conn)

            # íƒ€ì„ë¼ì¸ ì •ë³´ ìƒì„± (ê° í´ë¦½ì˜ ë©”íƒ€ë°ì´í„° í¬í•¨)
            timeline: list[dict[str, str | float | None]] = []
            current_time = 0.0
            for clip in video_clips:
                timeline.append(
                    {
                        "name": clip.name,
                        "duration": clip.duration,
                        "start": current_time,
                        "end": current_time + clip.duration,
                        "device": clip.device,
                        "shot_time": clip.shot_time,
                    }
                )
                current_time += clip.duration

            clips_json = json.dumps(timeline, ensure_ascii=False)

            # ì œëª©: ë””ë ‰í† ë¦¬ëª…
            title = None
            if targets:
                first_target = targets[0]
                title = first_target.name if first_target.is_dir() else first_target.parent.name
                if not title or title == ".":
                    title = output_path.stem

            today = date.today().isoformat()

            total_duration = sum(c.duration for c in video_clips)
            total_size = output_path.stat().st_size if output_path.exists() else 0

            # ì½˜ì†” ì¶œë ¥ìš© ìš”ì•½ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
            console_summary = generate_clip_summary(video_clips, groups=groups)
            # YouTube ì„¤ëª…ìš© (íƒ€ì„ìŠ¤íƒ¬í”„ + ì´¬ì˜ê¸°ê¸°)
            youtube_description = generate_youtube_description(video_clips, groups=groups)

            merge_job_id = repo.create(
                output_path=output_path,
                video_ids=video_ids,
                title=title,
                date=today,
                total_duration_seconds=total_duration,
                total_size_bytes=total_size,
                clips_info_json=clips_json,
                summary_markdown=youtube_description,
            )

        logger.debug("Merge job saved to database with summary")
        return console_summary, merge_job_id

    except Exception as e:
        logger.warning(f"Failed to save merge job to DB: {e}")
        return None, None


def _link_merge_job_to_project(project_name: str, merge_job_id: int) -> None:
    """ë³‘í•© ê²°ê³¼ë¥¼ í”„ë¡œì íŠ¸ì— ì—°ê²°í•œë‹¤.

    í”„ë¡œì íŠ¸ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±í•˜ê³ , merge_jobì„ ì—°ê²°í•œë‹¤.
    ë‚ ì§œ ë²”ìœ„ë„ ìë™ìœ¼ë¡œ ê°±ì‹ ëœë‹¤.

    Args:
        project_name: í”„ë¡œì íŠ¸ ì´ë¦„
        merge_job_id: merge_job ID
    """
    from tubearchive.database.repository import ProjectRepository

    try:
        with database_session() as conn:
            repo = ProjectRepository(conn)
            project = repo.get_or_create(project_name)
            if project.id is None:
                logger.warning("Project created but has no ID")
                return
            repo.add_merge_job(project.id, merge_job_id)
            logger.info(f"Merge job {merge_job_id} linked to project '{project_name}'")
            print(f"\nğŸ“ í”„ë¡œì íŠ¸ '{project_name}'ì— ë³‘í•© ê²°ê³¼ ì—°ê²°ë¨")
    except Exception as e:
        logger.warning(f"Failed to link merge job to project: {e}")


def upload_to_youtube(
    file_path: Path,
    title: str | None = None,
    description: str = "",
    privacy: str = "unlisted",
    publish_at: str | None = None,
    merge_job_id: int | None = None,
    playlist_ids: list[str] | None = None,
    chunk_mb: int | None = None,
    thumbnail: Path | None = None,
) -> str | None:
    """
    ì˜ìƒì„ YouTubeì— ì—…ë¡œë“œ.

    Args:
        file_path: ì—…ë¡œë“œí•  ì˜ìƒ íŒŒì¼ ê²½ë¡œ
        title: ì˜ìƒ ì œëª© (Noneì´ë©´ íŒŒì¼ëª… ì‚¬ìš©)
        description: ì˜ìƒ ì„¤ëª…
        privacy: ê³µê°œ ì„¤ì • (public, unlisted, private)
        publish_at: ì˜ˆì•½ ê³µê°œ ì‹œê°„ (ISO 8601 í˜•ì‹, ì„¤ì • ì‹œ privacyëŠ” privateë¡œ ìë™ ë³€ê²½)
        merge_job_id: DBì— ì €ì¥í•  MergeJob ID
        playlist_ids: ì¶”ê°€í•  í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì¶”ê°€ ì•ˆ í•¨)
        chunk_mb: ì—…ë¡œë“œ ì²­í¬ í¬ê¸° MB (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜/ê¸°ë³¸ê°’)
        thumbnail: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ê²½ë¡œ

    Returns:
        ì—…ë¡œë“œëœ YouTube ì˜ìƒ ID. ì‹¤íŒ¨ ì‹œ None.
    """
    from tubearchive.youtube.auth import YouTubeAuthError, get_authenticated_service
    from tubearchive.youtube.playlist import PlaylistError, add_to_playlist
    from tubearchive.youtube.uploader import (
        YouTubeUploader,
        YouTubeUploadError,
        validate_upload,
    )

    if not file_path.exists():
        raise FileNotFoundError(f"Video file not found: {file_path}")

    # ì—…ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦
    validation = validate_upload(file_path)
    print(f"\n{validation.get_summary()}")

    if not validation.is_valid:
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   - ì˜ìƒì„ ë” ì‘ì€ íŒŒíŠ¸ë¡œ ë¶„í• í•˜ì—¬ ì—…ë¡œë“œ")
        print("   - ë¹„íŠ¸ë ˆì´íŠ¸ë¥¼ ë‚®ì¶° ì¬ì¸ì½”ë”©")
        raise YouTubeUploadError("Video exceeds YouTube limits")

    if validation.warnings:
        # ê²½ê³ ê°€ ìˆìœ¼ë©´ ì‚¬ìš©ì í™•ì¸
        try:
            response = safe_input("\nê³„ì† ì—…ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
            if response not in ("y", "yes"):
                print("ì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return None
        except KeyboardInterrupt:
            print("\nì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return None

    # ì œëª© ê²°ì •: ì§€ì •ê°’ > íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)
    # YYYYMMDD í˜•ì‹ì„ 'YYYYë…„ Mì›” Dì¼'ë¡œ ë³€í™˜
    raw_title = title or file_path.stem
    video_title = format_youtube_title(raw_title)

    logger.info(f"Uploading to YouTube: {file_path}")
    logger.info(f"  Title: {video_title}")
    logger.info(f"  Privacy: {privacy}")

    # ì¸ì¦ ìƒíƒœ í™•ì¸
    from tubearchive.youtube.auth import check_auth_status

    status = check_auth_status()

    if not status.has_client_secrets:
        print("\nâŒ YouTube ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"\n{status.get_setup_guide()}")
        print("\nì„¤ì • ì™„ë£Œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        raise YouTubeAuthError("client_secrets.json not found")

    if not status.has_valid_token:
        print("\nğŸ” YouTube ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   ë¸Œë¼ìš°ì €ì—ì„œ Google ê³„ì • ì¸ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤...\n")

    try:
        # ì¸ì¦ (í† í° ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì—´ë¦¼)
        service = get_authenticated_service()

        # ì—…ë¡œë“œ
        uploader = YouTubeUploader(service, chunk_mb=chunk_mb)

        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì •
        file_size_bytes = file_path.stat().st_size
        file_size_mb = file_size_bytes / (1024 * 1024)
        bar_width = 30
        last_percent = -1

        def on_progress(percent: int) -> None:
            """ì—…ë¡œë“œ ì§„í–‰ë¥  ì½œë°± -- í”„ë¡œê·¸ë ˆìŠ¤ ë°” ê°±ì‹ ."""
            nonlocal last_percent
            if percent == last_percent:
                return  # ì¤‘ë³µ ì—…ë°ì´íŠ¸ ë°©ì§€
            last_percent = percent

            filled = int(bar_width * percent / 100)
            bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
            uploaded_mb = file_size_mb * percent / 100
            # ì¤„ ì „ì²´ë¥¼ ì§€ìš°ê³  ë‹¤ì‹œ ì¶œë ¥ (\033[K: ì»¤ì„œë¶€í„° ì¤„ ëê¹Œì§€ ì§€ì›€)
            sys.stdout.write(
                f"\r\033[KğŸ“¤ [{bar}] {percent:3d}% ({uploaded_mb:.1f}/{file_size_mb:.1f}MB)"
            )
            sys.stdout.flush()
            if percent >= 100:
                sys.stdout.write("\n")
                sys.stdout.flush()

        result = uploader.upload(
            file_path=file_path,
            title=video_title,
            description=description,
            privacy=privacy,
            publish_at=publish_at,
            on_progress=on_progress,
        )

        if thumbnail is not None:
            try:
                uploader.set_thumbnail(result.video_id, thumbnail)
                print("ğŸ–¼ï¸  ì¸ë„¤ì¼ ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Failed to set thumbnail for {result.video_id}: {e}")
                print(f"âš ï¸  ì¸ë„¤ì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        print("\nâœ… YouTube ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ¬ URL: {result.url}")
        if result.scheduled_publish_at:
            print(f"ğŸ“… ì˜ˆì•½ ê³µê°œ: {result.scheduled_publish_at}")

        # í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if playlist_ids:
            for pid in playlist_ids:
                try:
                    item_id = add_to_playlist(service, pid, result.video_id)
                    print(f"ğŸ“‹ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë¨: {pid} (item: {item_id})")
                except PlaylistError as e:
                    logger.warning(f"Failed to add to playlist {pid}: {e}")
                    print(f"âš ï¸ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì‹¤íŒ¨ ({pid}): {e}")

        # DBì— YouTube ID ì €ì¥
        if merge_job_id is not None:
            try:
                with database_session() as conn:
                    repo = MergeJobRepository(conn)
                    repo.update_youtube_id(merge_job_id, result.video_id)
                logger.debug(f"YouTube ID {result.video_id} saved to merge job {merge_job_id}")
            except Exception as e:
                logger.warning(f"Failed to save YouTube ID to DB: {e}")

        return result.video_id

    except YouTubeAuthError as e:
        logger.error(f"YouTube authentication failed: {e}")
        print(f"\nâŒ YouTube ì¸ì¦ ì‹¤íŒ¨: {e}")
        print("\nì„¤ì • ê°€ì´ë“œ: tubearchive --setup-youtube")
        raise
    except YouTubeUploadError as e:
        logger.error(f"YouTube upload failed: {e}")
        print(f"\nâŒ YouTube ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    return None


def cmd_setup_youtube() -> None:
    """
    --setup-youtube ì˜µì…˜ ì²˜ë¦¬.

    YouTube ì¸ì¦ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì„¤ì • ê°€ì´ë“œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    from tubearchive.youtube.auth import check_auth_status

    print("\nğŸ¬ YouTube ì—…ë¡œë“œ ì„¤ì • ìƒíƒœ\n")
    print("=" * 50)

    status = check_auth_status()
    print(status.get_setup_guide())

    print("=" * 50)

    # ë¸Œë¼ìš°ì € ì¸ì¦ì´ í•„ìš”í•˜ë©´ ë°”ë¡œ ì‹¤í–‰ ì œì•ˆ
    if status.needs_browser_auth:
        print("\nğŸ’¡ ì§€ê¸ˆ ë°”ë¡œ ì¸ì¦í•˜ë ¤ë©´:")
        print("   tubearchive --youtube-auth")
        print("   (ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©° Google ê³„ì • ì¸ì¦ì´ ì§„í–‰ë©ë‹ˆë‹¤)")


def cmd_youtube_auth() -> None:
    """
    --youtube-auth ì˜µì…˜ ì²˜ë¦¬.

    ë¸Œë¼ìš°ì €ë¥¼ ì—´ì–´ YouTube OAuth ì¸ì¦ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    from tubearchive.youtube.auth import (
        YouTubeAuthError,
        check_auth_status,
        get_client_secrets_path,
        get_token_path,
        run_auth_flow,
        save_credentials,
    )

    print("\nğŸ” YouTube ì¸ì¦ ì‹œì‘\n")

    # ë¨¼ì € ìƒíƒœ í™•ì¸
    status = check_auth_status()

    if status.has_valid_token:
        print("âœ… ì´ë¯¸ ì¸ì¦ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        print(f"   í† í° ìœ„ì¹˜: {status.token_path}")
        return

    if not status.has_client_secrets:
        print("âŒ client_secrets.jsonì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   í•„ìš”í•œ ìœ„ì¹˜: {status.client_secrets_path}")
        print("\nì„¤ì • ê°€ì´ë“œë¥¼ ë³´ë ¤ë©´: tubearchive --setup-youtube")
        raise YouTubeAuthError("client_secrets.json not found")

    # ë¸Œë¼ìš°ì € ì¸ì¦ ì‹¤í–‰
    print("ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ Google ê³„ì • ì¸ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
    print("   (ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤)\n")

    try:
        secrets_path = get_client_secrets_path()
        token_path = get_token_path()

        credentials = run_auth_flow(secrets_path)
        save_credentials(credentials, token_path)

        print("\nâœ… ì¸ì¦ ì™„ë£Œ!")
        print(f"   í† í° ì €ì¥ë¨: {token_path}")
        print("\nì´ì œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   tubearchive --upload ~/Videos/")
        print("   tubearchive --upload-only video.mp4")

    except Exception as e:
        logger.error(f"YouTube authentication failed: {e}")
        print(f"\nâŒ ì¸ì¦ ì‹¤íŒ¨: {e}")
        raise


def cmd_list_playlists() -> None:
    """
    --list-playlists ì˜µì…˜ ì²˜ë¦¬.

    ë‚´ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•˜ì—¬ IDì™€ í•¨ê»˜ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    from tubearchive.youtube.auth import get_authenticated_service
    from tubearchive.youtube.playlist import list_playlists

    print("\nğŸ“‹ ë‚´ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡\n")

    try:
        service = get_authenticated_service()
        playlists = list_playlists(service)

        if not playlists:
            print("í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"{'ë²ˆí˜¸':<4} {'ì œëª©':<40} {'ì˜ìƒìˆ˜':<8} ID")
        print("-" * 80)
        for i, pl in enumerate(playlists, 1):
            print(f"{i:<4} {pl.title:<40} {pl.item_count:<8} {pl.id}")

        print("-" * 80)
        print("\nğŸ’¡ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ:")
        print(f"   export {ENV_YOUTUBE_PLAYLIST}={playlists[0].id}")
        if len(playlists) > 1:
            ids = ",".join(pl.id for pl in playlists[:2])
            print(f"   export {ENV_YOUTUBE_PLAYLIST}={ids}  # ì—¬ëŸ¬ ê°œ")

    except Exception as e:
        logger.error(f"Failed to list playlists: {e}")
        print(f"\nâŒ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ìŠ¤ì½”í”„ ë¶€ì¡± ì—ëŸ¬ ì²˜ë¦¬
        if "insufficient" in str(e).lower() or "scope" in str(e).lower():
            from tubearchive.youtube.auth import get_token_path

            token_path = get_token_path()
            print("\nğŸ’¡ ê¶Œí•œì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í† í°ì„ ì‚­ì œí•˜ê³  ì¬ì¸ì¦í•˜ì„¸ìš”:")
            print(f"   rm {token_path}")
            print("   tubearchive --youtube-auth")
        raise


def _delete_build_records(conn: sqlite3.Connection, video_ids: list[int]) -> None:
    """ë¹Œë“œ ê´€ë ¨ ë ˆì½”ë“œ ì‚­ì œ (transcoding_jobs â†’ videos ìˆœì„œ).

    íŠ¸ëœìŠ¤ì½”ë”© ì‘ì—…ì„ ë¨¼ì € ì‚­ì œí•œ ë’¤ ì›ë³¸ ì˜ìƒ ë ˆì½”ë“œë¥¼ ì‚­ì œí•œë‹¤.
    ì™¸ë˜í‚¤ ì°¸ì¡° ìˆœì„œë¥¼ ì§€í‚¤ê¸° ìœ„í•´ transcoding_jobsë¥¼ ë¨¼ì € ì •ë¦¬í•œë‹¤.

    Args:
        conn: DB ì—°ê²°
        video_ids: ì‚­ì œí•  ì˜ìƒ ID ëª©ë¡
    """
    if not video_ids:
        return
    TranscodingJobRepository(conn).delete_by_video_ids(video_ids)
    VideoRepository(conn).delete_by_ids(video_ids)


def _interactive_select(items: Sequence[object], prompt: str) -> int | None:
    """
    ëŒ€í™”í˜• ëª©ë¡ ì„ íƒ.

    Args:
        items: ì„ íƒ ëŒ€ìƒ ëª©ë¡
        prompt: ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  í”„ë¡¬í”„íŠ¸

    Returns:
        ì„ íƒëœ ì¸ë±ìŠ¤(0-based) ë˜ëŠ” ì·¨ì†Œ ì‹œ None
    """
    try:
        choice = safe_input(prompt)
        if not choice or choice == "0":
            print("ì·¨ì†Œë¨")
            return None

        idx = int(choice) - 1
        if 0 <= idx < len(items):
            return idx

        print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        return None
    except ValueError:
        print("ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return None
    except KeyboardInterrupt:
        print("\nì·¨ì†Œë¨")
        return None


def _resolve_upload_thumbnail(
    explicit_thumbnail: Path | None,
    generated_thumbnail_paths: list[Path] | None = None,
) -> Path | None:
    """ì—…ë¡œë“œìš© ì¸ë„¤ì¼ ê²½ë¡œë¥¼ ê²°ì •í•œë‹¤.

    ìš°ì„ ìˆœìœ„:
    1. --set-thumbnail ì§€ì •ê°’
    2. ìƒì„±ëœ ì¸ë„¤ì¼ì´ 1ê°œë©´ ìë™ ì‚¬ìš©
    3. ìƒì„±ëœ ì¸ë„¤ì¼ì´ ì—¬ëŸ¬ ê°œë©´ ì¸í„°ë™í‹°ë¸Œ ì„ íƒ

    ì„ íƒì„ ê±´ë„ˆë›°ë©´ Noneì„ ë°˜í™˜í•œë‹¤.
    """
    if explicit_thumbnail is not None:
        return explicit_thumbnail

    if not generated_thumbnail_paths:
        return None

    if len(generated_thumbnail_paths) == 1:
        return generated_thumbnail_paths[0]

    print("\nì¸ë„¤ì¼ì„ ì„ íƒí•˜ì„¸ìš” (0: ê±´ë„ˆë›°ê¸°).")
    for i, path in enumerate(generated_thumbnail_paths, start=1):
        size_mb = path.stat().st_size / (1024 * 1024)
        print(f"  {i}. {path.name} ({size_mb:.1f}MB)")

    selected = _interactive_select(generated_thumbnail_paths, "ì„ íƒ: ")
    if selected is None:
        return None
    return generated_thumbnail_paths[selected]


def cmd_reset_build(path_arg: str) -> None:
    """``--reset-build`` ì˜µì…˜ ì²˜ë¦¬.

    ë³‘í•© ê¸°ë¡ê³¼ ê´€ë ¨ íŠ¸ëœìŠ¤ì½”ë”© ê¸°ë¡ì„ ì‚­ì œí•˜ì—¬ ë‹¤ì‹œ ë¹Œë“œí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

    Args:
        path_arg: íŒŒì¼ ê²½ë¡œ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ëŒ€í™”í˜• ëª©ë¡ì—ì„œ ì„ íƒ)
    """
    with database_session() as conn:
        repo = MergeJobRepository(conn)

        if path_arg:
            target_path = Path(path_arg).resolve()

            # merge_jobì—ì„œ video_ids ì¡°íšŒ â†’ ê´€ë ¨ ë ˆì½”ë“œ ì‚­ì œ
            merge_job = repo.get_by_output_path(target_path)
            if merge_job:
                _delete_build_records(conn, merge_job.video_ids)

            deleted = repo.delete_by_output_path(target_path)
            if deleted > 0:
                print(f"âœ… ë¹Œë“œ ê¸°ë¡ ì‚­ì œë¨: {target_path}")
                print("   ì´ì œ ë‹¤ì‹œ ë¹Œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âš ï¸ í•´ë‹¹ ê²½ë¡œì˜ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤: {target_path}")
        else:
            jobs = repo.get_all()
            if not jobs:
                print("ğŸ“‹ ë¹Œë“œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            print("\nğŸ“‹ ë¹Œë“œ ê¸°ë¡ ëª©ë¡")
            print("=" * 80)
            print(f"{'ë²ˆí˜¸':<4} {'ì œëª©':<30} {'ë‚ ì§œ':<12} {'YouTube':<10} ê²½ë¡œ")
            print("-" * 80)
            for i, job in enumerate(jobs, 1):
                title = (job.title or "-")[:28]
                job_date = job.date or "-"
                yt_status = "âœ… ì—…ë¡œë“œë¨" if job.youtube_id else "-"
                path = truncate_path(str(job.output_path), max_len=40)
                print(f"{i:<4} {title:<30} {job_date:<12} {yt_status:<10} {path}")
            print("=" * 80)

            idx = _interactive_select(jobs, "\nì‚­ì œí•  ë²ˆí˜¸ ì…ë ¥ (0: ì·¨ì†Œ): ")
            if idx is None:
                return

            job = jobs[idx]
            _delete_build_records(conn, job.video_ids)
            if job.id is not None:
                repo.delete(job.id)
            print(f"\nâœ… ë¹Œë“œ ê¸°ë¡ ì‚­ì œë¨: {job.title or job.output_path}")
            print("   ì´ì œ ë‹¤ì‹œ ë¹Œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def cmd_reset_upload(path_arg: str) -> None:
    """``--reset-upload`` ì˜µì…˜ ì²˜ë¦¬.

    YouTube ì—…ë¡œë“œ ê¸°ë¡ì„ ì´ˆê¸°í™”í•˜ì—¬ ë‹¤ì‹œ ì—…ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

    Args:
        path_arg: íŒŒì¼ ê²½ë¡œ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ëŒ€í™”í˜• ëª©ë¡ì—ì„œ ì„ íƒ)
    """
    with database_session() as conn:
        repo = MergeJobRepository(conn)

        if path_arg:
            target_path = Path(path_arg).resolve()
            merge_job = repo.get_by_output_path(target_path)
            if merge_job and merge_job.youtube_id:
                if merge_job.id is not None:
                    repo.clear_youtube_id(merge_job.id)
                print(f"âœ… ì—…ë¡œë“œ ê¸°ë¡ ì´ˆê¸°í™”ë¨: {target_path}")
                print(f"   ì´ì „ YouTube ID: {merge_job.youtube_id}")
                print("   ì´ì œ ë‹¤ì‹œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif merge_job:
                print(f"âš ï¸ ì´ë¯¸ ì—…ë¡œë“œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤: {target_path}")
            else:
                print(f"âš ï¸ í•´ë‹¹ ê²½ë¡œì˜ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤: {target_path}")
        else:
            jobs = repo.get_uploaded()
            if not jobs:
                print("ğŸ“‹ ì—…ë¡œë“œëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            print("\nğŸ“‹ ì—…ë¡œë“œëœ ì˜ìƒ ëª©ë¡")
            print("=" * 90)
            print(f"{'ë²ˆí˜¸':<4} {'ì œëª©':<30} {'ë‚ ì§œ':<12} {'YouTube ID':<15} ê²½ë¡œ")
            print("-" * 90)
            for i, job in enumerate(jobs, 1):
                title = (job.title or "-")[:28]
                job_date = job.date or "-"
                yt_id = job.youtube_id or "-"
                path = truncate_path(str(job.output_path), max_len=30)
                print(f"{i:<4} {title:<30} {job_date:<12} {yt_id:<15} {path}")
            print("=" * 90)

            idx = _interactive_select(jobs, "\nì´ˆê¸°í™”í•  ë²ˆí˜¸ ì…ë ¥ (0: ì·¨ì†Œ): ")
            if idx is None:
                return

            job = jobs[idx]
            if job.id is not None:
                repo.clear_youtube_id(job.id)
            print(f"\nâœ… ì—…ë¡œë“œ ê¸°ë¡ ì´ˆê¸°í™”ë¨: {job.title or job.output_path}")
            print(f"   ì´ì „ YouTube ID: {job.youtube_id}")
            print("   ì´ì œ ë‹¤ì‹œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def resolve_playlist_ids(playlist_args: list[str] | None) -> list[str]:
    """
    í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì¸ì ì²˜ë¦¬.

    ìš°ì„ ìˆœìœ„:
    1. --playlist ì˜µì…˜ì´ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ë¨ â†’ í•´ë‹¹ ê°’ ì‚¬ìš©
    2. --playlist ì˜µì…˜ ì—†ìŒ + í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ë¨ â†’ í™˜ê²½ ë³€ìˆ˜ ê°’ ì‚¬ìš©
    3. ë‘˜ ë‹¤ ì—†ìŒ â†’ ë¹ˆ ë¦¬ìŠ¤íŠ¸ (í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì•ˆ í•¨)

    Args:
        playlist_args: --playlist ì¸ì ê°’ ë¦¬ìŠ¤íŠ¸
            - None: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            - ë¹ˆ ë¬¸ìì—´ í¬í•¨: ëª©ë¡ì—ì„œ ì„ íƒ
            - ê¸°íƒ€: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ IDë¡œ ì‚¬ìš©

    Returns:
        í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš© ì•ˆ í•¨ ë˜ëŠ” ì·¨ì†Œ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ í™•ì¸
    if playlist_args is None:
        env_playlist = os.environ.get(ENV_YOUTUBE_PLAYLIST)
        if env_playlist:
            ids = [pid.strip() for pid in env_playlist.split(",") if pid.strip()]
            if ids:
                logger.info(f"Using playlists from env: {ids}")
                return ids
        return []

    # ë¹ˆ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ ì„ íƒ ëª¨ë“œ
    needs_selection = any(arg == "" for arg in playlist_args)
    direct_ids = [arg for arg in playlist_args if arg and arg != ""]

    if needs_selection:
        # í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì—ì„œ ì„ íƒ
        from tubearchive.youtube.auth import get_authenticated_service
        from tubearchive.youtube.playlist import list_playlists, select_playlist_interactive

        print("\nğŸ“‹ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        service = get_authenticated_service()
        playlists = list_playlists(service)

        selected = select_playlist_interactive(playlists)
        if selected:
            for pl in selected:
                print(f"   ì„ íƒë¨: {pl.title}")
            direct_ids.extend([pl.id for pl in selected])

    return direct_ids


def cmd_upload_only(args: argparse.Namespace) -> None:
    """
    --upload-only ì˜µì…˜ ì²˜ë¦¬.

    Args:
        args: íŒŒì‹±ëœ ì¸ì
    """
    file_path = Path(args.upload_only)

    if not file_path.exists():
        logger.error(f"File not found: {file_path}")
        sys.exit(1)

    # DBì—ì„œ MergeJob ì¡°íšŒ (ê²½ë¡œë¡œ ì°¾ê¸°)
    merge_job_id = None
    description = ""

    try:
        with database_session() as conn:
            merge_job = MergeJobRepository(conn).get_by_output_path(file_path)
            if merge_job:
                merge_job_id = merge_job.id
                if merge_job.summary_markdown:
                    description = merge_job.summary_markdown
                    logger.info("Using summary from database as description")
    except Exception as e:
        logger.warning(f"Failed to load merge job from DB: {e}")

    # í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    playlist_ids = resolve_playlist_ids(args.playlist)

    # ìŠ¤ì¼€ì¤„ ì²˜ë¦¬
    publish_at: str | None = None
    if hasattr(args, "schedule") and args.schedule:
        publish_at = parse_schedule_datetime(args.schedule)

    set_thumbnail = getattr(args, "set_thumbnail", None)
    set_thumbnail_path = _resolve_set_thumbnail_path(set_thumbnail)

    # ì—…ë¡œë“œ ì‹¤í–‰
    upload_to_youtube(
        file_path=file_path,
        title=args.upload_title,
        description=description,
        privacy=args.upload_privacy,
        publish_at=publish_at,
        merge_job_id=merge_job_id,
        playlist_ids=playlist_ids,
        chunk_mb=args.upload_chunk,
        thumbnail=set_thumbnail_path,
    )


def cmd_status() -> None:
    """``--status`` ì˜µì…˜ ì²˜ë¦¬: ì „ì²´ ì‘ì—… í˜„í™© ì¶œë ¥."""
    with database_session() as conn:
        video_repo = VideoRepository(conn)
        transcoding_repo = TranscodingJobRepository(conn)
        merge_repo = MergeJobRepository(conn)

        print("\nğŸ“Š TubeArchive ì‘ì—… í˜„í™©\n")

        # 1. ì§„í–‰ ì¤‘ì¸ íŠ¸ëœìŠ¤ì½”ë”© ì‘ì—…
        processing_jobs = transcoding_repo.get_active_with_paths(limit=10)

        if processing_jobs:
            print("ğŸ”„ ì§„í–‰ ì¤‘ì¸ íŠ¸ëœìŠ¤ì½”ë”©:")
            print("-" * 70)
            for tc_row in processing_jobs:
                path = Path(tc_row["original_path"]).name
                status = "â³ ëŒ€ê¸°" if tc_row["status"] == "pending" else "ğŸ”„ ì§„í–‰"
                progress = tc_row["progress_percent"] or 0
                print(f"  {status} [{progress:3d}%] {path}")
            print()

        # 2. ìµœê·¼ ë³‘í•© ì‘ì—…
        recent_merge_jobs = merge_repo.get_recent(limit=10)

        if recent_merge_jobs:
            print("ğŸ“ ìµœê·¼ ë³‘í•© ì‘ì—…:")
            print("-" * 90)
            print(f"{'ID':<4} {'ìƒíƒœ':<10} {'ì œëª©':<25} {'ë‚ ì§œ':<12} {'ê¸¸ì´':<10} {'YouTube':<12}")
            print("-" * 90)
            for job in recent_merge_jobs:
                title = (job.title or "-")[:23]
                job_date = job.date or "-"
                status_icon = STATUS_ICONS.get(job.status.value, job.status.value)
                duration_str = format_duration(job.total_duration_seconds or 0)
                yt_status = f"âœ… {job.youtube_id[:8]}..." if job.youtube_id else "- ë¯¸ì—…ë¡œë“œ"
                row_str = (
                    f"{job.id:<4} {status_icon:<10} {title:<25} {job_date:<12} {duration_str:<10}"
                )
                print(f"{row_str} {yt_status}")

            print("-" * 90)
        else:
            print("ğŸ“ ë³‘í•© ì‘ì—… ì—†ìŒ\n")

        # 3. í†µê³„ ìš”ì•½
        video_count = video_repo.count_all()
        total_jobs = merge_repo.count_all()
        uploaded_count = merge_repo.count_uploaded()

        print(
            f"\nğŸ“ˆ í†µê³„: ì˜ìƒ {video_count}ê°œ ë“±ë¡"
            f" | ë³‘í•© {total_jobs}ê±´ | ì—…ë¡œë“œ {uploaded_count}ê±´"
        )


def cmd_status_detail(job_id: int) -> None:
    """``--status-detail`` ì˜µì…˜ ì²˜ë¦¬: íŠ¹ì • ì‘ì—…ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        job_id: merge_job ID
    """
    with database_session() as conn:
        job = MergeJobRepository(conn).get_by_id(job_id)

        if not job:
            print(f"âŒ ì‘ì—… ID {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nğŸ“‹ ì‘ì—… ìƒì„¸ (ID: {job_id})\n")
        print("=" * 60)

        print(f"ğŸ“Œ ì œëª©: {job.title or '-'}")
        print(f"ğŸ“… ë‚ ì§œ: {job.date or '-'}")
        print(f"ğŸ“ ì¶œë ¥: {job.output_path}")
        print(f"ğŸ“Š ìƒíƒœ: {STATUS_ICONS.get(job.status.value, job.status.value)}")
        print(f"â±ï¸  ê¸¸ì´: {format_duration(job.total_duration_seconds or 0)}")
        print(f"ğŸ’¾ í¬ê¸°: {format_size(job.total_size_bytes or 0)}")

        if job.youtube_id:
            print(f"ğŸ¬ YouTube: https://youtu.be/{job.youtube_id}")
        else:
            print("ğŸ¬ YouTube: ë¯¸ì—…ë¡œë“œ")

        # í´ë¦½ ì •ë³´
        if job.clips_info_json:
            try:
                clips = json.loads(job.clips_info_json)
                print(f"\nğŸ“¹ í´ë¦½ ({len(clips)}ê°œ):")
                print("-" * 60)
                for i, clip in enumerate(clips, 1):
                    name = clip.get("name", "-")
                    clip_duration = clip.get("duration", 0)
                    device = clip.get("device", "-")
                    shot_time = clip.get("shot_time", "-")
                    print(f"  {i}. {name}")
                    print(f"     ê¸°ê¸°: {device} | ì´¬ì˜: {shot_time} | ê¸¸ì´: {clip_duration:.1f}s")
            except json.JSONDecodeError:
                pass

        print("=" * 60)


def _cmd_dry_run(validated_args: ValidatedArgs) -> None:
    """ì‹¤í–‰ ê³„íšë§Œ ì¶œë ¥í•˜ê³  ì‹¤ì œ íŠ¸ëœìŠ¤ì½”ë”©ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

    ì…ë ¥ íŒŒì¼ ëª©ë¡, ì¶œë ¥ ê²½ë¡œ, ê°ì¢… ì˜µì…˜ ì„¤ì •ê°’ì„ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€
    í˜•íƒœë¡œ ì½˜ì†”ì— í‘œì‹œí•œë‹¤. ``--dry-run`` í”Œë˜ê·¸ ì²˜ë¦¬ìš©.

    Args:
        validated_args: ê²€ì¦ëœ CLI ì¸ì
    """
    logger.info("Dry run mode - showing execution plan only")

    video_files = scan_videos(validated_args.targets)
    original_count = len(video_files)
    video_files = _apply_ordering(video_files, validated_args, allow_interactive=False)
    output_str = str(_resolve_output_path(validated_args))

    print("\n=== Dry Run Execution Plan ===")
    print(f"Input targets: {[str(t) for t in validated_args.targets]}")

    if original_count != len(video_files):
        print(f"Video files found: {original_count} (filtered to {len(video_files)})")
        if validated_args.exclude_patterns:
            print(f"  Exclude patterns: {validated_args.exclude_patterns}")
        if validated_args.include_only_patterns:
            print(f"  Include-only patterns: {validated_args.include_only_patterns}")
    else:
        print(f"Video files found: {len(video_files)}")

    if validated_args.sort_key != "time":
        print(f"Sort key: {validated_args.sort_key}")

    print_video_list(video_files, header="ìµœì¢… í´ë¦½ ìˆœì„œ")

    print(f"Output: {output_str}")
    print(f"Temp dir: {get_temp_dir()}")
    print(f"Resume enabled: {not validated_args.no_resume}")
    print(f"Keep temp files: {validated_args.keep_temp}")
    print(f"Parallel workers: {validated_args.parallel}")
    print(f"Denoise enabled: {validated_args.denoise}")
    print(f"Denoise level: {validated_args.denoise_level}")
    print(f"Normalize audio: {validated_args.normalize_audio}")
    print(f"Group sequences: {validated_args.group_sequences}")
    print(f"Fade duration: {validated_args.fade_duration}")
    if validated_args.stabilize:
        strength = validated_args.stabilize_strength
        crop = validated_args.stabilize_crop
        print(f"Stabilize: enabled (strength={strength}, crop={crop})")
    else:
        print("Stabilize: disabled")
    if validated_args.thumbnail:
        print(f"Thumbnail: enabled (quality={validated_args.thumbnail_quality})")
        if validated_args.thumbnail_timestamps:
            print(f"  timestamps: {validated_args.thumbnail_timestamps}")
        else:
            print("  timestamps: auto (10%, 33%, 50%)")
    if validated_args.bgm_path:
        print(f"BGM: {validated_args.bgm_path}")
        print(f"  volume: {validated_args.bgm_volume}")
        print(f"  loop: {validated_args.bgm_loop}")
    print("=" * 30)


def _upload_split_files(
    split_files: list[Path],
    title: str | None,
    clips_info_json: str | None,
    privacy: str,
    merge_job_id: int | None,
    playlist_ids: list[str] | None,
    chunk_mb: int | None,
    split_job_id: int | None = None,
    publish_at: str | None = None,
    thumbnail: Path | None = None,
) -> None:
    """ë¶„í•  íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ YouTubeì— ì—…ë¡œë“œí•œë‹¤.

    ê° íŒŒì¼ì— ëŒ€í•´ ì±•í„°ë¥¼ ë¦¬ë§¤í•‘í•˜ì—¬ ì„¤ëª…ì„ ìƒì„±í•˜ê³ ,
    ì œëª©ì— ``(Part N/M)`` í˜•ì‹ì„ ì¶”ê°€í•œë‹¤.
    ì¸ë„¤ì¼ì€ ëª¨ë“  íŒŒíŠ¸ì— ë™ì¼í•˜ê²Œ ì ìš©í•œë‹¤.

    Args:
        split_files: ë¶„í• ëœ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        title: ì›ë³¸ ì˜ìƒ ì œëª© (Noneì´ë©´ íŒŒì¼ëª… ì‚¬ìš©)
        clips_info_json: í´ë¦½ ë©”íƒ€ë°ì´í„° JSON ë¬¸ìì—´
        privacy: ê³µê°œ ì„¤ì •
        merge_job_id: MergeJob DB ID
        playlist_ids: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ëª©ë¡
        chunk_mb: ì—…ë¡œë“œ ì²­í¬ í¬ê¸° MB
        split_job_id: SplitJob DB ID (íŒŒíŠ¸ë³„ youtube_id ì €ì¥ìš©)
        publish_at: ì˜ˆì•½ ê³µê°œ ì‹œê°„ (ISO 8601 í˜•ì‹, ì„¤ì • ì‹œ privacyëŠ” privateë¡œ ìë™ ë³€ê²½)
        thumbnail: ì¸ë„¤ì¼ ì´ë¯¸ì§€ ê²½ë¡œ
    """
    from tubearchive.utils.summary_generator import (
        generate_split_youtube_description,
    )

    # clips_info_json â†’ ClipInfo ë¦¬ìŠ¤íŠ¸ ë³µì›
    video_clips: list[ClipInfo] = []
    if clips_info_json:
        try:
            raw = json.loads(clips_info_json)
            for item in raw:
                video_clips.append(
                    ClipInfo(
                        name=item.get("name", ""),
                        duration=float(item.get("duration", 0)),
                        device=item.get("device"),
                        shot_time=item.get("shot_time"),
                    )
                )
        except (json.JSONDecodeError, KeyError, TypeError):
            logger.warning("Failed to parse clips_info_json for split upload")

    # ê° ë¶„í•  íŒŒì¼ì˜ ì‹¤ì œ ê¸¸ì´ ì¡°íšŒ
    split_durations = [probe_duration(f) for f in split_files]

    total = len(split_files)
    for i, split_file in enumerate(split_files):
        part_title = f"{title} (Part {i + 1}/{total})" if title else None

        # ì±•í„° ë¦¬ë§¤í•‘ëœ ì„¤ëª… ìƒì„±
        description = ""
        if video_clips and any(d > 0 for d in split_durations):
            try:
                description = generate_split_youtube_description(
                    video_clips=video_clips,
                    split_durations=split_durations,
                    part_index=i,
                )
            except Exception as e:
                logger.warning(f"Failed to generate split description: {e}")

        print(f"\nğŸ“¤ Part {i + 1}/{total} ì—…ë¡œë“œ: {split_file.name}")
        try:
            # merge_job_id=None: ë¶„í•  íŒŒíŠ¸ëŠ” merge_jobì˜ youtube_idë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ
            video_id = upload_to_youtube(
                file_path=split_file,
                title=part_title,
                description=description,
                privacy=privacy,
                publish_at=publish_at,
                merge_job_id=None,
                playlist_ids=playlist_ids,
                chunk_mb=chunk_mb,
                thumbnail=thumbnail,
            )
            # íŒŒíŠ¸ë³„ youtube_idë¥¼ split_jobì— ì €ì¥
            if video_id and split_job_id is not None:
                try:
                    with database_session() as conn:
                        split_repo = SplitJobRepository(conn)
                        split_repo.append_youtube_id(split_job_id, video_id)
                except Exception as e:
                    logger.warning(f"Failed to save youtube_id for part {i + 1}: {e}")
        except Exception as e:
            logger.error(f"Part {i + 1}/{total} upload failed: {e}")
            print(f"  âš ï¸  Part {i + 1} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue


def _get_or_create_project_playlist(
    project_name: str,
    merge_job_id: int,
    privacy: str = "unlisted",
) -> str | None:
    """í”„ë¡œì íŠ¸ ì „ìš© YouTube í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•˜ê±°ë‚˜ ìƒì„±í•œë‹¤.

    DBì— ì €ì¥ëœ playlist_idê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ ,
    ì—†ìœ¼ë©´ YouTubeì— ìƒˆ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ DBì— ì €ì¥í•œë‹¤.

    Args:
        project_name: í”„ë¡œì íŠ¸ ì´ë¦„
        merge_job_id: merge_job ID (í”„ë¡œì íŠ¸ ì¡°íšŒìš©)
        privacy: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ê³µê°œ ì„¤ì •

    Returns:
        í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    from tubearchive.database.repository import ProjectRepository

    try:
        # Phase 1: DB ì¡°íšŒ â€” í”„ë¡œì íŠ¸ì™€ ê¸°ì¡´ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ í™•ì¸
        with database_session() as conn:
            repo = ProjectRepository(conn)
            project_ids = repo.get_project_ids_for_merge_job(merge_job_id)
            if not project_ids:
                return None

            project = repo.get_by_id(project_ids[0])
            if project is None or project.id is None:
                return None

            if project.playlist_id:
                logger.info(f"Reusing project playlist: {project.playlist_id}")
                return project.playlist_id

            project_id = project.id

        # Phase 2: YouTube API í˜¸ì¶œ â€” DB ì„¸ì…˜ ë°–ì—ì„œ ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ
        from tubearchive.youtube.auth import get_authenticated_service
        from tubearchive.youtube.playlist import create_playlist

        service = get_authenticated_service()
        playlist_id = create_playlist(
            service,
            title=project_name,
            description=f"TubeArchive í”„ë¡œì íŠ¸: {project_name}",
            privacy=privacy,
        )

        # Phase 3: DB ì—…ë°ì´íŠ¸ â€” ìƒì„±ëœ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ì €ì¥
        with database_session() as conn:
            repo = ProjectRepository(conn)
            repo.update_playlist_id(project_id, playlist_id)

        print(f"  ğŸ“‹ í”„ë¡œì íŠ¸ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ìƒì„±ë¨: {project_name}")
        return playlist_id

    except Exception as e:
        logger.warning(f"Failed to get/create project playlist: {e}")
        return None


def _upload_after_pipeline(
    output_path: Path,
    args: argparse.Namespace,
    notifier: Notifier | None = None,
    publish_at: str | None = None,
    generated_thumbnail_paths: list[Path] | None = None,
    explicit_thumbnail: Path | None = None,
) -> None:
    """íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ YouTube ì—…ë¡œë“œë¥¼ ìˆ˜í–‰í•œë‹¤.

    DBì—ì„œ ìµœì‹  merge_jobì„ ì¡°íšŒí•˜ì—¬ ì œëª©Â·ì„¤ëª…ì„ ê°€ì ¸ì˜¨ ë’¤,
    ë¶„í•  íŒŒì¼ì´ ìˆìœ¼ë©´ ìˆœì°¨ ì—…ë¡œë“œ, ì—†ìœ¼ë©´ ë‹¨ì¼ ì—…ë¡œë“œí•œë‹¤.

    Args:
        output_path: ì—…ë¡œë“œí•  ë³‘í•© ì˜ìƒ íŒŒì¼ ê²½ë¡œ
        args: ì›ë³¸ CLI ì¸ì (playlist, upload_privacy, upload_chunk ë“±)
        notifier: ì•Œë¦¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (Noneì´ë©´ ì•Œë¦¼ ë¹„í™œì„±í™”)
        publish_at: ì˜ˆì•½ ê³µê°œ ì‹œê°„ (ì´ë¯¸ ê²€ì¦ëœ ê°’, ì¬íŒŒì‹±í•˜ì§€ ì•ŠìŒ)
        generated_thumbnail_paths: ì¸ë„¤ì¼ í›„ë³´ ê²½ë¡œ ëª©ë¡ (ìƒì„±ëœ ì¸ë„¤ì¼)
        explicit_thumbnail: --set-thumbnailì—ì„œ ì§€ì •í•œ ì¸ë„¤ì¼ ê²½ë¡œ
    """
    print("\nğŸ“¤ YouTube ì—…ë¡œë“œ ì‹œì‘...")

    thumbnail = _resolve_upload_thumbnail(
        explicit_thumbnail=explicit_thumbnail,
        generated_thumbnail_paths=generated_thumbnail_paths,
    )
    if thumbnail is not None:
        logger.info(
            "Using thumbnail for upload: %s",
            getattr(thumbnail, "name", str(thumbnail)),
        )
    else:
        logger.info("No thumbnail selected for upload.")

    merge_job_id = None
    title = None
    description = ""
    clips_info_json: str | None = None
    try:
        with database_session() as conn:
            repo = MergeJobRepository(conn)
            job = repo.get_latest()
            if job:
                merge_job_id = job.id
                title = job.title
                description = job.summary_markdown or ""
                clips_info_json = job.clips_info_json
    except Exception as e:
        logger.warning(f"Failed to get merge job: {e}")

    playlist_ids = resolve_playlist_ids(args.playlist)

    # í”„ë¡œì íŠ¸ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±/ì‚¬ìš©
    project_name = getattr(args, "project", None)
    if project_name and merge_job_id is not None:
        project_playlist_id = _get_or_create_project_playlist(
            project_name, merge_job_id, privacy=args.upload_privacy
        )
        if project_playlist_id and project_playlist_id not in playlist_ids:
            playlist_ids.append(project_playlist_id)

    # ë¶„í•  íŒŒì¼ í™•ì¸
    split_files: list[Path] = []
    split_job_id: int | None = None
    if merge_job_id is not None:
        try:
            with database_session() as conn:
                split_repo = SplitJobRepository(conn)
                split_jobs = split_repo.get_by_merge_job_id(merge_job_id)
                for sj in split_jobs:
                    existing = [f for f in sj.output_files if f.exists()]
                    if existing:
                        split_files.extend(existing)
                        split_job_id = sj.id
        except Exception as e:
            logger.warning(f"Failed to get split jobs: {e}")

    if split_files:
        _upload_split_files(
            split_files=split_files,
            title=title,
            clips_info_json=clips_info_json,
            privacy=args.upload_privacy,
            merge_job_id=merge_job_id,
            playlist_ids=playlist_ids,
            chunk_mb=args.upload_chunk,
            split_job_id=split_job_id,
            publish_at=publish_at,
            thumbnail=thumbnail,
        )
    else:
        upload_to_youtube(
            file_path=output_path,
            title=title,
            description=description,
            privacy=args.upload_privacy,
            publish_at=publish_at,
            merge_job_id=merge_job_id,
            playlist_ids=playlist_ids,
            chunk_mb=args.upload_chunk,
            thumbnail=thumbnail,
        )

    # ì•Œë¦¼: ì—…ë¡œë“œ ì™„ë£Œ
    if notifier:
        from tubearchive.notification import upload_complete_event

        # DBì—ì„œ youtube_id ì¡°íšŒ
        youtube_id = ""
        if merge_job_id is not None:
            try:
                with database_session() as conn:
                    repo = MergeJobRepository(conn)
                    job = repo.get_by_id(merge_job_id)
                    if job and job.youtube_id:
                        youtube_id = job.youtube_id
            except Exception:
                logger.debug("ì•Œë¦¼ìš© youtube_id ì¡°íšŒ ì‹¤íŒ¨", exc_info=True)
        notifier.notify(
            upload_complete_event(
                video_title=title or output_path.stem,
                youtube_id=youtube_id,
            )
        )


def cmd_init_config() -> None:
    """
    --init-config ì˜µì…˜ ì²˜ë¦¬.

    ê¸°ë³¸ ì„¤ì • íŒŒì¼(config.toml) í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from tubearchive.config import generate_default_config, get_default_config_path

    config_path = get_default_config_path()

    if config_path.exists():
        response = safe_input(f"ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {config_path}\në®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() not in ("y", "yes"):
            print("ì·¨ì†Œë¨")
            return

    config_path.parent.mkdir(parents=True, exist_ok=True)
    config_path.write_text(generate_default_config())
    print(f"ì„¤ì • íŒŒì¼ ìƒì„±ë¨: {config_path}")


def main() -> None:
    """CLI ì§„ì…ì .

    ì¸ìë¥¼ íŒŒì‹±í•˜ê³  ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•œ ë’¤, ìš”ì²­ëœ ì„œë¸Œì»¤ë§¨ë“œë¥¼
    ì ì ˆí•œ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë¡œ ë¼ìš°íŒ…í•œë‹¤. ì„œë¸Œì»¤ë§¨ë“œê°€ ì§€ì •ë˜ì§€ ì•Šì€
    ê¸°ë³¸ ë™ì‘ì€ :func:`run_pipeline` (íŠ¸ëœìŠ¤ì½”ë”© + ë³‘í•©).
    """
    parser = create_parser()
    args = parser.parse_args()

    # --init-config ì²˜ë¦¬ (ê°€ì¥ ë¨¼ì €, ë¡œê¹…/ì„¤ì • ë¡œë“œ ì „)
    if args.init_config:
        cmd_init_config()
        return

    # ì„¤ì • íŒŒì¼ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì ìš©
    from tubearchive.config import apply_config_to_env, load_config

    config_path = Path(args.config) if args.config else None
    config = load_config(config_path)
    apply_config_to_env(config)

    # --notify-test ì²˜ë¦¬ (ì„œë¸Œì»¤ë§¨ë“œ ì „)
    if getattr(args, "notify_test", False):
        setup_logging(args.verbose)
        from tubearchive.notification import Notifier as _Notifier

        test_notifier = _Notifier(config.notification)
        if not test_notifier.has_providers:
            print("í™œì„±í™”ëœ ì•Œë¦¼ ì±„ë„ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("config.tomlì˜ [notification] ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”.")
            return
        results = test_notifier.test_notification()
        for provider_name, success in results.items():
            icon = "OK" if success else "FAIL"
            status = "ì„±ê³µ" if success else "ì‹¤íŒ¨"
            print(f"  [{icon}] {provider_name}: {status}")
        return

    # upload_privacy: CLI > config > "unlisted"
    if args.upload_privacy is None:
        args.upload_privacy = config.youtube.upload_privacy or "unlisted"

    setup_logging(args.verbose)

    notifier: Notifier | None = None

    try:
        # --setup-youtube ì˜µì…˜ ì²˜ë¦¬ (ì„¤ì • ê°€ì´ë“œ)
        if args.setup_youtube:
            cmd_setup_youtube()
            return

        # --youtube-auth ì˜µì…˜ ì²˜ë¦¬ (ë¸Œë¼ìš°ì € ì¸ì¦)
        if args.youtube_auth:
            cmd_youtube_auth()
            return

        # --list-playlists ì˜µì…˜ ì²˜ë¦¬ (í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ëª©ë¡)
        if args.list_playlists:
            cmd_list_playlists()
            return

        # --reset-build ì˜µì…˜ ì²˜ë¦¬ (ë¹Œë“œ ê¸°ë¡ ì´ˆê¸°í™”)
        if args.reset_build is not None:
            cmd_reset_build(args.reset_build)
            return

        # --reset-upload ì˜µì…˜ ì²˜ë¦¬ (ì—…ë¡œë“œ ê¸°ë¡ ì´ˆê¸°í™”)
        if args.reset_upload is not None:
            cmd_reset_upload(args.reset_upload)
            return

        # --project-list ì˜µì…˜ ì²˜ë¦¬ (í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ)
        if args.project_list:
            from tubearchive.commands.project import cmd_project_list

            cmd_project_list(output_json=args.json)
            return

        # --project-detail ì˜µì…˜ ì²˜ë¦¬ (í”„ë¡œì íŠ¸ ìƒì„¸ ì¡°íšŒ)
        if args.project_detail is not None:
            from tubearchive.commands.project import cmd_project_detail

            cmd_project_detail(args.project_detail, output_json=args.json)
            return

        # --status-detail ì˜µì…˜ ì²˜ë¦¬ (ì‘ì—… ìƒì„¸ ì¡°íšŒ)
        if args.status_detail is not None:
            cmd_status_detail(args.status_detail)
            return

        # --status ì˜µì…˜ ì²˜ë¦¬ (ì‘ì—… í˜„í™© ì¡°íšŒ)
        if args.status == CATALOG_STATUS_SENTINEL:
            cmd_status()
            return

        # --period ë‹¨ë… ì‚¬ìš© ê²½ê³ 
        if args.period and not args.stats:
            logger.warning("--period ì˜µì…˜ì€ --statsì™€ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")

        # --stats ì˜µì…˜ ì²˜ë¦¬ (í†µê³„ ëŒ€ì‹œë³´ë“œ)
        if args.stats:
            from tubearchive.commands.stats import cmd_stats as _cmd_stats

            with database_session() as conn:
                _cmd_stats(conn, period=args.period)
            return

        # --catalog / --search ì˜µì…˜ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ì¡°íšŒ)
        if (args.json or args.csv) and not (
            args.catalog
            or args.search is not None
            or args.device is not None
            or normalize_status_filter(args.status) is not None
        ):
            raise ValueError("--json/--csv ì˜µì…˜ì€ --catalog ë˜ëŠ” --searchì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”.")

        if (
            args.catalog
            or args.search is not None
            or args.device is not None
            or normalize_status_filter(args.status) is not None
        ):
            cmd_catalog(args)
            return

        # --upload-only ì˜µì…˜ ì²˜ë¦¬ (ì—…ë¡œë“œë§Œ)
        if args.upload_only:
            cmd_upload_only(args)
            return

        # configì˜ device_lutsë¥¼ validate_argsì— ì „ë‹¬í•˜ì—¬ ì´ˆê¸°í™” ì‹œ ì£¼ì…
        cfg_device_luts = config.color_grading.device_luts or None
        validated_args = validate_args(args, device_luts=cfg_device_luts)

        if validated_args.dry_run:
            _cmd_dry_run(validated_args)
            return

        # Notifier ì´ˆê¸°í™”
        if validated_args.notify:
            from tubearchive.notification import Notifier as _Notifier

            notifier = _Notifier(config.notification)
            if notifier.has_providers:
                logger.info("ì•Œë¦¼ ì‹œìŠ¤í…œ í™œì„±í™” (%dê°œ ì±„ë„)", notifier.provider_count)

        pipeline_generated_thumbnail_paths: list[Path] = []
        output_path = run_pipeline(
            validated_args,
            notifier=notifier,
            generated_thumbnail_paths=pipeline_generated_thumbnail_paths,
        )
        print("\nâœ… ì™„ë£Œ!")
        print(f"ğŸ“¹ ì¶œë ¥ íŒŒì¼: {output_path}")

        if validated_args.upload:
            _upload_after_pipeline(
                output_path,
                args,
                notifier=notifier,
                publish_at=validated_args.schedule,
                generated_thumbnail_paths=pipeline_generated_thumbnail_paths,
                explicit_thumbnail=validated_args.set_thumbnail,
            )

    except FileNotFoundError as e:
        logger.error(str(e))
        sys.exit(1)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)
    except KeyboardInterrupt:
        logger.info("\nInterrupted by user")
        sys.exit(130)
    except Exception as e:
        # ì—ëŸ¬ ì•Œë¦¼
        if notifier is not None:
            from tubearchive.notification import error_event

            notifier.notify(error_event(error_message=str(e), stage="pipeline"))
        logger.exception(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
